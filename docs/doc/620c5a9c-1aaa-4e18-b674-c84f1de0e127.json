{
    "summary": "The code constructs a deep learning model for image classification, utilizing ViT and MBConv layers. It incorporates activation functions, checks pretrained weights, and offers the option to use FilmMaxVit class based on configuration. The real-time video processing model uses ViT and RT1 with configurable options and train/eval methods.",
    "details": [
        {
            "comment": "The code is importing necessary libraries and defines helper functions for packing and unpacking data, handling default values, and creating sinusoidal position embeddings. This section of the code also includes a function for generating sinusoidal position embeddings in 1D. The code imports specific conditioners from another module and uses the 'cast_tuple' function to handle tuples of various lengths.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":0-46",
            "content": "from functools import partial\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, einsum\nfrom typing import List, Optional, Callable, Tuple\nfrom beartype import beartype\nfrom einops import pack, unpack, repeat, reduce, rearrange\nfrom einops.layers.torch import Rearrange, Reduce\nfrom classifier_free_guidance_pytorch import (\n    TextConditioner as FilmTextConditioner,\n    AttentionTextConditioner as FilmAttentionTextConditioner,\n    classifier_free_guidance,\n)\n# helpers\ndef exists(val):\n    return val is not None\ndef default(val, d):\n    return val if exists(val) else d\ndef cast_tuple(val, length=1):\n    return val if isinstance(val, tuple) else ((val,) * length)\ndef pack_one(x, pattern):\n    return pack([x], pattern)\ndef unpack_one(x, ps, pattern):\n    return unpack(x, ps, pattern)[0]\n# sinusoidal positions\ndef posemb_sincos_1d(\n    seq, dim, temperature=10000, device=None, dtype=torch.float32\n):\n    n = torch.arange(seq, device=device)\n    omega = torch.arange(dim // 2, device=device) / (dim // 2 - 1)"
        },
        {
            "comment": "This code defines a Residual, LayerNorm, and FeedForward classes for neural network layers. The main function calculates position embeddings using trigonometric functions and returns it in the specified dtype.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":47-90",
            "content": "    omega = 1.0 / (temperature**omega)\n    n = n[:, None] * omega[None, :]\n    pos_emb = torch.cat((n.sin(), n.cos()), dim=1)\n    return pos_emb.type(dtype)\n# helper classes\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(x) + x\nclass LayerNorm(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(dim))\n        self.register_buffer(\"beta\", torch.zeros(dim))\n    def forward(self, x):\n        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult=4, dropout=0.0):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        self.norm = LayerNorm(dim)\n        self.net = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout),\n        )\n    def forward(self, x, cond_fn=None):"
        },
        {
            "comment": "The provided code defines two classes: SqueezeExcitation and MBConvResidual. The SqueezeExcitation class implements a squeeze-and-excitation module, which applies a gate to the input based on global context, reducing dimensionality before applying a sigmoid function to create the gate. The MBConvResidual class is a residual block using the function (fn) and DropSample layer to apply dropout if specified.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":91-136",
            "content": "        x = self.norm(x)\n        if exists(cond_fn):\n            # adaptive layernorm\n            x = cond_fn(x)\n        return self.net(x)\n# MBConv\nclass SqueezeExcitation(nn.Module):\n    def __init__(self, dim, shrinkage_rate=0.25):\n        super().__init__()\n        hidden_dim = int(dim * shrinkage_rate)\n        self.gate = nn.Sequential(\n            Reduce(\"b c h w -> b c\", \"mean\"),\n            nn.Linear(dim, hidden_dim, bias=False),\n            nn.SiLU(),\n            nn.Linear(hidden_dim, dim, bias=False),\n            nn.Sigmoid(),\n            Rearrange(\"b c -> b c 1 1\"),\n        )\n    def forward(self, x):\n        return x * self.gate(x)\nclass MBConvResidual(nn.Module):\n    def __init__(self, fn, dropout=0.0):\n        super().__init__()\n        self.fn = fn\n        self.dropsample = Dropsample(dropout)\n    def forward(self, x):\n        out = self.fn(x)\n        out = self.dropsample(out)\n        return out + x\nclass Dropsample(nn.Module):\n    def __init__(self, prob=0):\n        super().__init__()\n        self.prob = prob"
        },
        {
            "comment": "The code defines a forward pass function for a neural network layer and a MBConv function that takes input and output dimensions, downsampling, expansion rate, shrinkage rate, and dropout rate as arguments. It creates a sequential model consisting of convolutional layers, batch normalization layers, GELU activation functions, and a SqueezeExcitation module.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":138-181",
            "content": "    def forward(self, x):\n        device = x.device\n        if self.prob == 0.0 or (not self.training):\n            return x\n        keep_mask = (\n            torch.FloatTensor(\n                (x.shape[0], 1, 1, 1), device=device\n            ).uniform_()\n            > self.prob\n        )\n        return x * keep_mask / (1 - self.prob)\ndef MBConv(\n    dim_in,\n    dim_out,\n    *,\n    downsample,\n    expansion_rate=4,\n    shrinkage_rate=0.25,\n    dropout=0.0,\n):\n    hidden_dim = int(expansion_rate * dim_out)\n    stride = 2 if downsample else 1\n    net = nn.Sequential(\n        nn.Conv2d(dim_in, hidden_dim, 1),\n        nn.BatchNorm2d(hidden_dim),\n        nn.GELU(),\n        nn.Conv2d(\n            hidden_dim,\n            hidden_dim,\n            3,\n            stride=stride,\n            padding=1,\n            groups=hidden_dim,\n        ),\n        nn.BatchNorm2d(hidden_dim),\n        nn.GELU(),\n        SqueezeExcitation(hidden_dim, shrinkage_rate=shrinkage_rate),\n        nn.Conv2d(hidden_dim, dim_out, 1),\n        nn.BatchNorm2d(dim_out),"
        },
        {
            "comment": "The code defines a class named \"Attention\" for multi-head self-attention. It initializes layer normalization, splits the input into multiple heads, scales them, and passes through linear layers. It also includes softmax activation for attention scores, dropout regularization, and relative positional bias using an embedding layer.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":182-223",
            "content": "    )\n    if dim_in == dim_out and not downsample:\n        net = MBConvResidual(net, dropout=dropout)\n    return net\n# attention related classes\nclass Attention(nn.Module):\n    def __init__(self, dim, dim_head=32, dropout=0.0, window_size=7):\n        super().__init__()\n        assert (\n            dim % dim_head\n        ) == 0, \"dimension should be divisible by dimension per head\"\n        self.norm = LayerNorm(dim)\n        self.heads = dim // dim_head\n        self.scale = dim_head**-0.5\n        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.attend = nn.Sequential(\n            nn.Softmax(dim=-1), nn.Dropout(dropout)\n        )\n        self.to_out = nn.Sequential(\n            nn.Linear(dim, dim, bias=False), nn.Dropout(dropout)\n        )\n        # relative positional bias\n        self.rel_pos_bias = nn.Embedding(\n            (2 * window_size - 1) ** 2, self.heads\n        )\n        pos = torch.arange(window_size)\n        grid = torch.stack(torch.meshgrid(pos, pos, indexing=\"ij\"))\n        grid = rearrange(grid, \"c i j -> (i j) c\")"
        },
        {
            "comment": "This code snippet is initializing the parameters for a multi-head attention layer in PyTorch. It rearranges input dimensions, projects input into queries (Q), keys (K), and values (V) matrices, splits the input into multiple heads, scales the Q matrix by a factor of 'self.scale', and performs some rearrangements on the K and V matrices. This is commonly used in transformer models for natural language processing tasks such as text classification or machine translation.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":224-271",
            "content": "        rel_pos = rearrange(grid, \"i ... -> i 1 ...\") - rearrange(\n            grid, \"j ... -> 1 j ...\"\n        )\n        rel_pos += window_size - 1\n        rel_pos_indices = (\n            rel_pos * torch.tensor([2 * window_size - 1, 1])\n        ).sum(dim=-1)\n        self.register_buffer(\n            \"rel_pos_indices\", rel_pos_indices, persistent=False\n        )\n    def forward(self, x):\n        (\n            batch,\n            height,\n            width,\n            window_height,\n            window_width,\n            _,\n            device,\n            h,\n        ) = (\n            *x.shape,\n            x.device,\n            self.heads,\n        )\n        x = self.norm(x)\n        # flatten\n        x = rearrange(x, \"b x y w1 w2 d -> (b x y) (w1 w2) d\")\n        # project for queries, keys, values\n        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n        # split heads\n        q, k, v = map(\n            lambda t: rearrange(t, \"b n (h d ) -> b h n d\", h=h),\n            (q, k, v),\n        )\n        # scale\n        q = q * self.scale"
        },
        {
            "comment": "This code appears to belong to a FilmMaxVit model, an image classification network. It seems to be performing attention-based feature extraction from input images and then rearranging the output for further processing or returning. The `FilmViTConfig` class is used to store the configuration settings for this particular model.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":273-317",
            "content": "        # sim\n        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k)\n        # add positional bias\n        bias = self.rel_pos_bias(self.rel_pos_indices)\n        sim = sim + rearrange(bias, \"i j h -> h i j\")\n        # attention\n        attn = self.attend(sim)\n        # aggregate\n        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n        # merge heads\n        out = rearrange(\n            out,\n            \"b h (w1 w2) d -> b w1 w2 (h d)\",\n            w1=window_height,\n            w2=window_width,\n        )\n        # combine heads out\n        out = self.to_out(out)\n        return rearrange(\n            out, \"(b x y) ... -> b x y ...\", x=height, y=width\n        )\nclass FilmViTConfig:\n    \"\"\"Configuration class to store the configuration of a `FilmMaxVit`.\"\"\"\n    def __init__(\n        self,\n        num_classes=1000,  # 1000 for ImageNet\n        input_channels=3,\n        stem_channels_in=64,  # Number of stem channels\n        dim_head=32,  # Attention head dimension\n        block_channel_ins: List = [\n            64,"
        },
        {
            "comment": "This code defines a function to construct a MaxVit architecture, which is a type of Vision Transformer. It takes parameters such as number of classes, input channels, and dimensions of the head, and returns an instance of the constructed network. The MaxVit architecture also includes optional film layers and options for dropout and normalization.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":318-349",
            "content": "            128,\n            256,\n            512,\n        ],  # Number of channels for each ViT block\n        block_layers=[\n            2,\n            2,\n            5,\n            2,\n        ],  # Number of layers for each ViT block\n        window_size=7,  # Partition size\n        mbconv_expansion_rate=4,\n        mbconv_shrinkage_rate=0.25,  # MBConv squeeze ratio\n        dropout=0.1,\n        norm_layer: nn.Module = None,\n        activation_layer=nn.GELU,\n        stochastic_depth_prob=0.2,\n        pretrained=False,\n    ):\n        \"\"\"\n        Constructs a MaxVit architecture with optional film layers from\n        `MaxVit: Multi-Axis Vision Transformer <https://arxiv.org/abs/2204.01697>`_.\n            Parameters\n            ----------\n            num_classes : int\n                Number of classes for the classification task\n            input_channels : int\n                Number of input channels\n            stem_channels_in : int\n                Number of stem channels\n            dim_head : int\n                Dimension of the head"
        },
        {
            "comment": "This code defines a class with several parameters including the number of classes, input channels, block channel inputs and layers, window size, MBConv expansion rate and squeeze ratio, dropout probability, normalization layer, activation layer, and stochastic depth probability. The class likely represents a deep learning model for an image classification task using ViT blocks with MBConv layers.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":350-376",
            "content": "            block_channel_ins : List\n                Number of channels for each ViT block\n            block_layers : List\n                Number of layers for each ViT block\n            window_size : int\n                Partition size\n            mbconv_expansion_rate : int\n                MBConv expansion rate\n            mbconv_shrinkage_rate : float\n                MBConv squeeze ratio\n            dropout : float\n                Dropout probability\n            norm_layer : nn.Module\n                Normalization layer\n            activation_layer : nn.Module\n                Activation layer\n            stochastic_depth_prob : float\n                Stochastic depth probability\n        \"\"\"\n        self.num_classes = num_classes\n        self.input_channels = input_channels\n        self.stem_channels_in = stem_channels_in\n        self.block_channel_ins = block_channel_ins\n        self.block_layers = block_layers\n        self.dim_head = dim_head\n        self.stem_channels_in = stem_channels_in\n        self.window_size = window_size"
        },
        {
            "comment": "The code initializes class attributes including expansion and shrinkage rates for MbConv, dropout rate, and a norm layer with default parameters if None is passed. It also sets the activation layer and checks if pretrained weights are used. Lastly, it checks the stochastic depth probability. The code then defines another class, FilmMaxVit, which takes in a FilmViTConfig object and initializes lists for input and output channel numbers for each ViT block based on the configuration.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":377-405",
            "content": "        self.mbconv_expansion_rate = mbconv_expansion_rate\n        self.mbconv_shrinkage_rate = mbconv_shrinkage_rate\n        self.dropout = dropout\n        self.norm_layer = norm_layer\n        if self.norm_layer is None:\n            self.norm_layer = partial(\n                nn.BatchNorm2d, eps=1e-3, momentum=0.99\n            )\n        self.activation_layer = activation_layer\n        self.pretrained = pretrained\n        self.stochastic_depth_prob = stochastic_depth_prob\nclass FilmMaxVit(nn.Module):\n    def __init__(\n        self,\n        config: FilmViTConfig,\n    ):\n        super().__init__()\n        assert isinstance(config.block_layers, tuple | list), (\n            \"depth needs to be tuple if integers indicating number of\"\n            \" transformer blocks at that stage\"\n        )\n        # List of number of input and output channels for each ViT block.\n        in_channels: List = [\n            config.stem_channels_in\n        ] + config.block_channel_ins[:-1]\n        out_channels: List = config.block_channel_ins"
        },
        {
            "comment": "This code initializes the conditioned hidden dimensions and embed dimension for a model. It also checks if pretrained weights are used, and if so, it loads the Vision Transformer (ViT) model's stem, classifier, blocks, and layers. The purpose is to create an instance of the model with pre-initialized parameters.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":407-433",
            "content": "        # Condition after each layer starting with the input to the stem block.\n        self.cond_hidden_dims = [\n            config.stem_channels_in\n        ]  # Used by FilmTextConditioner\n        for block_in_channels, block_layers in zip(\n            out_channels, config.block_layers\n        ):\n            for _ in range(block_layers):\n                self.cond_hidden_dims.append(block_in_channels)\n        self.cond_hidden_dims = self.cond_hidden_dims[\n            :-1\n        ]  # Don't condition on last embedding.\n        self.embed_dim = out_channels[-1]\n        if config.pretrained:\n            from torchvision.models import maxvit_t, MaxVit_T_Weights\n            self._vit = maxvit_t(weights=MaxVit_T_Weights.DEFAULT)\n            self.conv_stem = self._vit.stem\n            self.mlp_head = self._vit.classifier\n            self.layers = nn.ModuleList([])\n            for block in self._vit.blocks:\n                for layer in block.layers:\n                    self.layers.append(layer)\n            return\n        # convolutional stem"
        },
        {
            "comment": "This code initializes a deep learning model for image classification. It consists of a convolutional stem, followed by multiple blocks of MBConv layers with varying channels and downsampling options. The model's configuration is defined in the `config` object.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":434-468",
            "content": "        self.conv_stem = nn.Sequential(\n            nn.Conv2d(\n                config.input_channels,\n                config.stem_channels_in,\n                3,\n                stride=2,\n                padding=1,\n            ),\n            nn.Conv2d(\n                config.stem_channels_in,\n                config.stem_channels_in,\n                3,\n                padding=1,\n            ),\n        )\n        self.layers = nn.ModuleList([])\n        for (\n            block_channels_in,\n            block_channels_out,\n            block_num_layers,\n        ) in zip(in_channels, out_channels, config.block_layers):\n            for i in range(block_num_layers):\n                layer_channels_in = (\n                    block_channels_in\n                    if i == 0\n                    else block_channels_out\n                )\n                layer = nn.Sequential(\n                    MBConv(\n                        layer_channels_in,\n                        block_channels_out,\n                        downsample=(i == 0),\n                        expansion_rate=config.mbconv_expansion_rate,"
        },
        {
            "comment": "This code section is creating a block of a neural network. It includes various layers like Conv, Rearrange, Residual, Attention, and FeedForward. The layers are configured with specific parameters like shrinkage rate, window size, block channels out, dim head, dropout, etc., taken from the config file. The code rearranges the dimensions of input tensors as needed for different operations within the block.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":469-492",
            "content": "                        shrinkage_rate=config.mbconv_shrinkage_rate,\n                    ),\n                    Rearrange(\n                        \"b d (x w1) (y w2) -> b x y w1 w2 d\",\n                        w1=config.window_size,\n                        w2=config.window_size,\n                    ),  # block-like attention\n                    Residual(\n                        Attention(\n                            dim=block_channels_out,\n                            dim_head=config.dim_head,\n                            dropout=config.dropout,\n                            window_size=config.window_size,\n                        )\n                    ),\n                    Residual(\n                        FeedForward(\n                            dim=block_channels_out,\n                            dropout=config.dropout,\n                        )\n                    ),\n                    Rearrange(\"b x y w1 w2 d -> b d (x w1) (y w2)\"),\n                    Rearrange(\n                        \"b d (w1 x) (w2 y) -> b x y w1 w2 d\","
        },
        {
            "comment": "The code defines a Transformer block with grid-like attention, followed by residual connections and feedforward layers. It then appends the block to 'self.layers' and creates an MLP head for classification using Reduce, LayerNorm, and Linear layers.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":493-520",
            "content": "                        w1=config.window_size,\n                        w2=config.window_size,\n                    ),  # grid-like attention\n                    Residual(\n                        Attention(\n                            dim=block_channels_out,\n                            dim_head=config.dim_head,\n                            dropout=config.dropout,\n                            window_size=config.window_size,\n                        )\n                    ),\n                    Residual(\n                        FeedForward(\n                            dim=block_channels_out,\n                            dropout=config.dropout,\n                        )\n                    ),\n                    Rearrange(\"b x y w1 w2 d -> b d (w1 x) (w2 y)\"),\n                )\n                self.layers.append(layer)\n        # mlp head out\n        self.mlp_head = nn.Sequential(\n            Reduce(\"b d h w -> b d\", \"mean\"),\n            LayerNorm(self.embed_dim),\n            nn.Linear(self.embed_dim, config.num_classes, bias=False),"
        },
        {
            "comment": "The function \"forward\" takes input 'x', optional texts, conditional functions, and other parameters. It applies convolutional layers followed by a Transformer attention module to process the input data. The function returns either the output or embeddings depending on the return_embeddings parameter.\n\nThe \"TransformerAttention\" class initializes an attention module with specified dimensions, number of heads, dropout rate, etc. It applies layer normalization and optionally normalizes the context dimension as well.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":521-573",
            "content": "        )\n    @beartype\n    def forward(\n        self,\n        x,\n        texts: Optional[List[str]] = None,\n        cond_fns: Optional[Tuple[Callable, ...]] = None,\n        cond_drop_prob=0.0,\n        return_embeddings=False,\n    ):\n        x = self.conv_stem(x)\n        cond_fns = iter(default(cond_fns, []))\n        for stage in self.layers:\n            cond_fn = next(cond_fns, None)\n            if exists(cond_fn):\n                x = cond_fn(x)\n            x = stage(x)\n        if return_embeddings:\n            return x\n        return self.mlp_head(x)\n# attention\nclass TransformerAttention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        causal=False,\n        dim_head=64,\n        dim_context=None,\n        heads=8,\n        norm_context=False,\n        dropout=0.1,\n    ):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim_head**-0.5\n        self.causal = causal\n        inner_dim = dim_head * heads\n        dim_context = default(dim_context, dim)\n        self.norm = LayerNorm(dim)\n        self.context_norm = ("
        },
        {
            "comment": "This code initializes a Multi-Head Attention layer. It includes LayerNorm for normalization, nn.Linear for transformations, and nn.Sequential for sequential operations. The forward function applies the attention mechanism to input 'x' and optionally context, considering masking and conditional functions if provided.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":574-615",
            "content": "            LayerNorm(dim_context) if norm_context else nn.Identity()\n        )\n        self.attn_dropout = nn.Dropout(dropout)\n        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n        self.to_kv = nn.Linear(dim_context, dim_head * 2, bias=False)\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim, bias=False), nn.Dropout(dropout)\n        )\n    def forward(\n        self,\n        x,\n        context=None,\n        mask=None,\n        attn_bias=None,\n        attn_mask=None,\n        cond_fn: Optional[Callable] = None,\n    ):\n        x.shape[0]\n        if exists(context):\n            context = self.context_norm(context)\n        kv_input = default(context, x)\n        x = self.norm(x)\n        if exists(cond_fn):\n            # adaptive layer-norm\n            x = cond_fn(x)\n        q, k, v = self.to_q(x), *self.to_kv(kv_input).chunk(2, dim=-1)\n        q = rearrange(q, \"b n (h d) -> b h n d\", h=self.heads)\n        q = q * self.scale\n        sim = einsum(\"b h i d, b j d -> b h i j\", q, k)\n        if exists(attn_bias):"
        },
        {
            "comment": "The code defines a Transformer class that takes in various dimensions and parameters for the attention mechanism. It applies an attentional mask to control the flow of information, applies causal masking for sequence modeling tasks, performs multi-head self-attention, and includes dropout regularization. The resulting output is then passed through a linear layer to get the final output.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":616-656",
            "content": "            sim = sim + attn_bias\n        if exists(attn_mask):\n            sim = sim.masked_fill(\n                ~attn_mask, -torch.finfo(sim.dtype).max\n            )\n        if exists(mask):\n            mask = rearrange(mask, \"b j -> b 1 1 j\")\n            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n        if self.causal:\n            i, j = sim.shape[-2:]\n            causal_mask = torch.ones(\n                (i, j), dtype=torch.bool, device=x.device\n            ).triu(j - i + 1)\n            sim = sim.masked_fill(\n                causal_mask, -torch.finfo(sim.dtype).max\n            )\n        attn = sim.softmax(dim=-1)\n        attn = self.attn_dropout(attn)\n        out = einsum(\"b h i j, b j d -> b h i d\", attn, v)\n        out = rearrange(out, \"b h n d -> b n (h d)\")\n        return self.to_out(out)\n@beartype\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=64,\n        heads=8,\n        depth=6,\n        attn_dropout=0.0,\n        ff_dropout=0.0,\n    ):\n        super().__init__()"
        },
        {
            "comment": "The code defines a class called `RTX` which appears to be a transformer network layer. It has a method `forward()` that takes input `x`, optional condition functions `cond_fns`, and an attention mask `attn_mask`. The `forward()` method applies the attention and feed-forward layers sequentially, adding the output to the input. There's also a class called `TokenLearner` which may be used for generating attention maps.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":657-697",
            "content": "        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(\n                nn.ModuleList(\n                    [\n                        TransformerAttention(\n                            dim=dim, heads=heads, dropout=attn_dropout\n                        ),\n                        FeedForward(dim=dim, dropout=ff_dropout),\n                    ]\n                )\n            )\n    def forward(\n        self,\n        x,\n        cond_fns: Optional[Tuple[Callable, ...]] = None,\n        attn_mask=None,\n    ):\n        cond_fns = iter(default(cond_fns, []))\n        for attn, ff in self.layers:\n            x = (\n                attn(\n                    x,\n                    attn_mask=attn_mask,\n                    cond_fn=next(cond_fns, None),\n                )\n                + x\n            )\n            x = ff(x, cond_fn=next(cond_fns, None)) + x\n        return x\n# token learner module\nclass TokenLearner(nn.Module):\n    \"\"\"\n    https://arxiv.org/abs/2106.11297\n    using the 1.1 version with the MLP (2 dense layers with gelu) for generating attention map"
        },
        {
            "comment": "This code defines a class that initializes an RTX network with given dimensions, feature scaling factor, number of output tokens, and layers. It utilizes convolutional layers and GELU activation for the network architecture. The forward function performs operations on input data to compute output for the network.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":698-736",
            "content": "    \"\"\"\n    def __init__(\n        self, *, dim, ff_mult=2, num_output_tokens=8, num_layers=2\n    ):\n        super().__init__()\n        inner_dim = dim * ff_mult * num_output_tokens\n        self.num_output_tokens = num_output_tokens\n        self.net = nn.Sequential(\n            nn.Conv2d(\n                dim * num_output_tokens,\n                inner_dim,\n                1,\n                groups=num_output_tokens,\n            ),\n            nn.GELU(),\n            nn.Conv2d(\n                inner_dim,\n                num_output_tokens,\n                1,\n                groups=num_output_tokens,\n            ),\n        )\n    def forward(self, x):\n        x, ps = pack_one(x, \"* c h w\")\n        x = repeat(\n            x, \"b c h w -> b (g c) h w\", g=self.num_output_tokens\n        )\n        attn = self.net(x)\n        attn = rearrange(attn, \"b g h w -> b 1 g h w\")\n        x = rearrange(\n            x, \"b (g c) h w -> b c g h w\", g=self.num_output_tokens\n        )\n        x = reduce(x * attn, \"b c g h w -> b c g\", \"mean\")\n        x = unpack_one(x, ps, \"* c n\")"
        },
        {
            "comment": "This code defines a configuration class, RT1Config, for the `RT1` model. It contains parameters such as number of actions, action bins, transformer block depth, and more. These settings can be customized to control the behavior and performance of the `RT1` model.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":737-768",
            "content": "        return x\n# Robotic Transformer\nclass RT1Config:\n    def __init__(\n        self,\n        num_actions=11,\n        action_bins=256,\n        depth=6,\n        heads=8,\n        dim_head=64,\n        token_learner_ff_mult=2,\n        token_learner_num_layers=2,\n        token_learner_num_output_tokens=8,\n        cond_drop_prob=0.2,\n        use_attn_conditioner=False,\n    ):\n        \"\"\"Configuration class to store the configuration of a `RT1`.\n        Args:\n            num_actions (int): Number of actions for the classification task\n            action_bins (int): Number of bins for each action\n            depth (int): Number of transformer blocks\n            heads (int): Number of heads for the transformer\n            dim_head (int): Dimension of the head\n            token_learner_ff_mult (int): Multiplier for the token learner\n            token_learner_num_layers (int): Number of layers for the token learner\n            token_learner_num_output_tokens (int): Number of output tokens for the token learner\n            cond_drop_prob (float): Dropout probability"
        },
        {
            "comment": "The code defines a class called RT1 that takes in a configuration, a Vision-Language model (vit), and optional conditioner kwargs. The class initializes the vit module and sets the number of vit stages based on the condition hidden dimensions. It also uses a FilmAttentionTextConditioner if use_attn_conditioner is set to True, otherwise it uses FilmTextConditioner.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":769-803",
            "content": "            use_attn_conditioner (bool): Whether to use the attention conditioner\n        \"\"\"\n        self.num_actions = num_actions\n        self.action_bins = action_bins\n        self.depth = depth\n        self.heads = heads\n        self.dim_head = dim_head\n        self.token_learner_ff_mult = token_learner_ff_mult\n        self.token_learner_num_layers = token_learner_num_layers\n        self.token_learner_num_output_tokens = (\n            token_learner_num_output_tokens\n        )\n        self.cond_drop_prob = cond_drop_prob\n        self.use_attn_conditioner = use_attn_conditioner\n@beartype\nclass RT1(nn.Module):\n    def __init__(\n        self,\n        config: RT1Config,\n        vit: FilmMaxVit,\n        conditioner_kwargs: dict = dict(),\n    ):\n        super().__init__()\n        self.vit = vit\n        self.num_vit_stages = len(vit.cond_hidden_dims)\n        film_layer = (\n            FilmAttentionTextConditioner\n            if config.use_attn_conditioner\n            else FilmTextConditioner\n        )\n        self.conditioner = film_layer("
        },
        {
            "comment": "This code initializes a conditional transformer model. It sets the hidden dimensions, hiddens_channel_first (alternating True/False), and cond_drop_prob from config values. The TokenLearner is initialized with specified dimensions and configuration options. The number of learned tokens, transformer depth, and cond_drop_prob are also set. Lastly, a Transformer object is created using the provided dimensions, dim_head, heads, and depth.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":804-838",
            "content": "            hidden_dims=(\n                *tuple(vit.cond_hidden_dims),\n                *((vit.embed_dim,) * config.depth * 2),\n            ),\n            hiddens_channel_first=(\n                *((True,) * self.num_vit_stages),\n                *((False,) * config.depth * 2),\n            ),\n            cond_drop_prob=config.cond_drop_prob,\n            **conditioner_kwargs,\n        )\n        self.token_learner = TokenLearner(\n            dim=vit.embed_dim,\n            ff_mult=config.token_learner_ff_mult,\n            num_output_tokens=config.token_learner_num_output_tokens,\n            num_layers=config.token_learner_num_layers,\n        )\n        self.num_learned_tokens = (\n            config.token_learner_num_output_tokens\n        )\n        self.transformer_depth = config.depth\n        self.transformer = Transformer(\n            dim=vit.embed_dim,\n            dim_head=config.dim_head,\n            heads=config.heads,\n            depth=config.depth,\n        )\n        self.cond_drop_prob = config.cond_drop_prob\n        self.to_logits = nn.Sequential("
        },
        {
            "comment": "This code defines a forward pass for a neural network that takes in video data and optional text inputs. The video passes through a series of transformations, including LayerNorm and linear operations, before being rearranged. The conditioner function generates conditional functions for the Vision Transformer (VIT) stages and the Transformer depth. These conditional functions are then used within the forward pass to guide the processing of the video data based on the text inputs.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":839-873",
            "content": "            LayerNorm(vit.embed_dim),\n            nn.Linear(\n                vit.embed_dim, config.num_actions * config.action_bins\n            ),\n            Rearrange(\"... (a b) -> ... a b\", b=config.action_bins),\n        )\n    @classifier_free_guidance\n    def forward(\n        self,\n        video,\n        texts: Optional[List[str]] = None,\n        cond_drop_prob=0.0,\n    ):\n        depth = self.transformer_depth\n        cond_drop_prob = default(cond_drop_prob, self.cond_drop_prob)\n        frames, device = video.shape[2], video.device\n        cond_fns = self.conditioner(\n            texts,\n            cond_drop_prob=cond_drop_prob,\n            repeat_batch=(\n                *((frames,) * self.num_vit_stages),\n                *((1,) * self.transformer_depth * 2),\n            ),\n        )\n        vit_cond_fns, transformer_cond_fns = (\n            cond_fns[: -(depth * 2)],\n            cond_fns[-(depth * 2) :],\n        )\n        video = rearrange(video, \"b c f h w -> b f c h w\")\n        images, packed_shape = pack_one(video, \"* c h w\")"
        },
        {
            "comment": "The code is performing token learning and feature extraction using a Vision-Language Model (VLM) called \"vit\". It then applies rearrangement, attention masking, and sinusoidal positional embedding to the learned tokens for further processing.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":875-912",
            "content": "        tokens = self.vit(\n            images,\n            texts=texts,\n            cond_fns=vit_cond_fns,\n            cond_drop_prob=cond_drop_prob,\n            return_embeddings=True,\n        )\n        tokens = unpack_one(tokens, packed_shape, \"* c h w\")\n        learned_tokens = self.token_learner(tokens)\n        learned_tokens = rearrange(\n            learned_tokens, \"b f c n -> b (f n) c\"\n        )\n        # causal attention mask\n        attn_mask = torch.ones(\n            (frames, frames), dtype=torch.bool, device=device\n        ).triu(1)\n        attn_mask = repeat(\n            attn_mask,\n            \"i j -> (i r1) (j r2)\",\n            r1=self.num_learned_tokens,\n            r2=self.num_learned_tokens,\n        )\n        # sinusoidal positional embedding\n        pos_emb = posemb_sincos_1d(\n            frames,\n            learned_tokens.shape[-1],\n            dtype=learned_tokens.dtype,\n            device=learned_tokens.device,\n        )\n        learned_tokens = learned_tokens + repeat(\n            pos_emb, \"n d -> (n r) d\", r=self.num_learned_tokens"
        },
        {
            "comment": "This code defines a class for real-time video processing using Vision Transformers (ViT) and Reinforcement Learning (RT1) models. It has train and eval methods that compute logits for videos and instructions using the RT1 model in respective modes.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":913-954",
            "content": "        )\n        # attention\n        attended_tokens = self.transformer(\n            learned_tokens,\n            cond_fns=transformer_cond_fns,\n            attn_mask=~attn_mask,\n        )\n        pooled = reduce(\n            attended_tokens, \"b (f n) d -> b f d\", \"mean\", f=frames\n        )\n        logits = self.to_logits(pooled)\n        return logits\nclass RTX1(nn.Module):\n    \"\"\"\n    A class for real-time video processing using Vision Transformers (ViT) and Reinforcement Learning (RT1) models.\n    ...\n    Attributes\n    ----------\n    vit : FilmMaxVit\n        a Vision Transformer model\n    model : RT1\n        a reinforcement learning model\n    Methods\n    -------\n    train(video, instructions):\n        Computes the logits for the given video and instructions using the RT1 model in training mode.\n    eval(video, instructions, cond_scale=1.0):\n        Computes the logits for the given video and instructions using the RT1 model in evaluation mode.\n    \"\"\"\n    def __init__(\n        self,\n        rt1_config: RT1Config = None,"
        },
        {
            "comment": "The code is a constructor for the RTX1 object, which initializes attributes from optional configuration objects for RT1 and ViT models. It sets default configurations if not specified and can be used to train or evaluate the model based on inputs and conditional scale.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":955-992",
            "content": "        vit_config: FilmViTConfig = None,\n    ):\n        \"\"\"\n        Constructs all the necessary attributes for the RTX1 object.\n        Parameters\n        ----------\n        rt1_config : RT1Config, optional\n            a configuration object for the RT1 model (default is None)\n        vit_config : FilmViTConfig, optional\n            a configuration object for the ViT model (default is None)\n        Example:\n        import torch\n        from rtx import RTX1\n        model = RTX1()\n        video = torch.randn(2, 3, 6, 224, 224)\n        instructions = [\"bring me that apple sitting on the table\", \"please pass the butter\"]\n        # compute the train logits\n        train_logits = model.train(video, instructions)\n        # set the model to evaluation mode\n        model.model.eval()\n        # compute the eval logits with a conditional scale of 3\n        eval_logits = model.run(video, instructions, cond_scale=3.0)\n        print(eval_logits.shape)\n        \"\"\"\n        super().__init__()\n        if rt1_config is None:\n            rt1_config = RT1Config()"
        },
        {
            "comment": "This code snippet is from a file named \"rtx1.py\". It checks if vit_config is None and initializes it as FilmViTConfig() if so. Then, it creates an instance of FilmMaxVit with the vit_config. The RT1 model is then initialized with rt1_config and the previously created FilmMaxVit instance. The train method computes logits for video and instructions using the RT1 model in training mode. If an exception occurs during training, a RuntimeError is raised. The run method computes logits for video and instructions using the RT1 model in evaluation mode.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":993-1027",
            "content": "        if vit_config is None:\n            vit_config = FilmViTConfig()\n        self.vit = FilmMaxVit(vit_config)\n        self.model = RT1(\n            config=rt1_config,\n            vit=self.vit,\n        )\n    def train(self, video, instructions):\n        \"\"\"\n        Computes the logits for the given video and instructions using the RT1 model in training mode.\n        Parameters\n        ----------\n        video : torch.Tensor\n            a tensor containing the video data\n        instructions : torch.Tensor\n            a tensor containing the instructions\n        Returns\n        -------\n        torch.Tensor\n            a tensor containing the computed logits\n        \"\"\"\n        try:\n            train_logits = self.model(video, instructions)\n            return train_logits\n        except Exception as e:\n            raise RuntimeError(\"Error in training: {}\".format(e))\n    def run(self, video, instructions, cond_scale=1.0):\n        \"\"\"\n        Computes the logits for the given video and instructions using the RT1 model in evaluation mode."
        },
        {
            "comment": "This function takes video and instruction data as input, along with an optional conditional scale factor. It applies the model's evaluation mode and returns a tensor of computed logits after processing the given inputs through the model. In case of any exception during this process, it raises a RuntimeError with the specific error message.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx1.py\":1029-1053",
            "content": "        Parameters\n        ----------\n        video : torch.Tensor\n            a tensor containing the video data\n        instructions : torch.Tensor\n            a tensor containing the instructions\n        cond_scale : float, optional\n            a scale factor for the conditional scaling (default is 1.0)\n        Returns\n        -------\n        torch.Tensor\n            a tensor containing the computed logits\n        \"\"\"\n        try:\n            self.model.eval()\n            # shape => 2, 3, 6, 224, 224\n            eval_logits = self.model(\n                video, instructions, cond_scale=cond_scale\n            )\n            return eval_logits\n        except Exception as e:\n            raise RuntimeError(\"Error in evaluation: {}\".format(e))"
        }
    ]
}