{
    "summary": "RT-X is a Pytorch implementation of RT-1 and RT-2 models, offering RTX-1 and RTX-2 architectures, supporting 7D vector output, installable via pip, and requiring cloning repository for testing. The code snippet covers integrating EfficientNetFilm into RTX-1, creating training scripts, using HuggingFace's RTX-2 dataset, and checking project board tasks.",
    "details": [
        {
            "comment": "RT-X is a Pytorch implementation of the RT-1-X and RT-2-X models from the \"Open X-Embodiment\" paper. It features RTX-1 and RTX-2 model architectures. The RTX-2 output is text tokens, but can be modified for 7D vector output like RTX-1. Install with `pip install rtx-torch` and use the RTX1 by providing text and video inputs to train the model. Efficient Net integration is ongoing.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/README.md\":0-38",
            "content": "[![Multi-Modality](agorabanner.png)](https://discord.gg/qUtxnK2NMf)\n# RT-X\nPytorch implementation of the models RT-1-X and RT-2-X from the paper: \"Open X-Embodiment: Robotic Learning Datasets and RT-X Models\".\nHere we implement both model architectures, RTX-1 and RTX-2\n[Paper Link](https://robotics-transformer-x.github.io/)\n- The RTX-2 Implementation does not natively output for simplicity a 7 dimensional vector but rather text tokens, if you wanted to output 7 dimensional vector you could implement the same token learner as in RTX1\n# Appreciation\n* Lucidrains\n* Agorians\n# Install\n`pip install rtx-torch `\n# Usage\n## RTX1\n- RTX1 Usage takes in text and videos\n- Does not use Efficient Net yet, we're integrating it now then the implementation will be complete\n- Uses SOTA transformer architecture\n```python\nimport torch\nfrom rtx.rtx1 import RTX1\nmodel = RTX1()\nvideo = torch.randn(2, 3, 6, 224, 224)\ninstructions = [\"bring me that apple sitting on the table\", \"please pass the butter\"]\n# compute the train logits\ntrain_logits = model.train(video, instructions)"
        },
        {
            "comment": "In this code, the model is set to evaluation mode and computes eval logits using a conditional scale of 3. The shape of the resulting eval_logits is then printed.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/README.md\":40-84",
            "content": "# set the model to evaluation mode\nmodel.model.eval()\n# compute the eval logits with a conditional scale of 3\neval_logits = model.run(video, instructions, cond_scale=3.0)\nprint(eval_logits.shape)\n```\n## RTX-2\n- RTX-2 takes in images and text and interleaves them to form multi-modal sentences and outputs text tokens not a 7 dimensional vector of x,y,z,roll,pitch,yaw,and gripper\n```python\nimport torch\nfrom rtx import RTX2\n# usage\nimg = torch.randn(1, 3, 256, 256)\ntext = torch.randint(0, 20000, (1, 1024))\nmodel = RTX2()\noutput = model(img, text)\nprint(output)\n```\n## EfficientNetFilm\n- Extracts the feature from the given image\n```python\nfrom rtx import EfficientNetFilm\nmodel = EfficientNetFilm(\"efficientnet-b0\", 10)\nout = model(\"img.jpeg\")\n```\n# Model Differences from the Paper Implementation\n## RT-1\nThe main difference here is the substitution of a Film-EfficientNet backbone (pre-trained EfficientNet-B3 with Film layers inserted) with a MaxViT model.\n# Tests\nI created a single tests file that uses pytest to run tests"
        },
        {
            "comment": "This code provides instructions to run tests on the RT-X modules, RTX1 and RTX2, along with EfficientNetFil. It also mentions the license and citation information for the Open X-Embodiment project. To execute the tests, clone the repository, install requirements using pip, and then run `pytest tests.py`.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/README.md\":84-95",
            "content": " on all the modules, RTX1, RTX2, EfficientNetFil, first git clone and get into the repository, install the requirements.txt with pip then run this:\n`pytest tests.py`\n# License\nMIT\n# Citations\n```bibtex\n@misc{open_x_embodiment_rt_x_2023,\ntitle={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},\nauthor = {Open X-Embodiment Collaboration and Abhishek Padalkar and Acorn Pooley and Ajinkya Jain and Alex Bewley and Alex Herzog and Alex Irpan and Alexander Khazatsky and Anant Rai and Anikait Singh and Anthony Brohan and Antonin Raffin and Ayzaan Wahid and Ben Burgess-Limerick and Beomjoon Kim and Bernhard Sch\u00f6lkopf and Brian Ichter and Cewu Lu and Charles Xu and Chelsea Finn and Chenfeng Xu and Cheng Chi and Chenguang Huang and Christine Chan and Chuer Pan and Chuyuan Fu and Coline Devin and Danny Driess and Deepak Pathak and Dhruv Shah and Dieter B\u00fcchler and Dmitry Kalashnikov and Dorsa Sadigh and Edward Johns and Federico Ceola and Fei Xia and Freek Stulp and Gaoyue Zhou and Gaurav "
        },
        {
            "comment": "This code appears to be a list of authors for a scientific paper or project, potentially in alphabetical order by last name. The names are separated by \"and\" and there's no clear indication that this is part of a larger codebase.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/README.md\":95-95",
            "content": "S. Sukhatme and Gautam Salhotra and Ge Yan and Giulio Schiavi and Hao Su and Hao-Shu Fang and Haochen Shi and Heni Ben Amor and Henrik I Christensen and Hiroki Furuta and Homer Walke and Hongjie Fang and Igor Mordatch and Ilija Radosavovic and Isabel Leal and Jacky Liang and Jaehyung Kim and Jan Schneider and Jasmine Hsu and Jeannette Bohg and Jeffrey Bingham and Jiajun Wu and Jialin Wu and Jianlan Luo and Jiayuan Gu and Jie Tan and Jihoon Oh and Jitendra Malik and Jonathan Tompson and Jonathan Yang and Joseph J. Lim and Jo\u00e3o Silv\u00e9rio and Junhyek Han and Kanishka Rao and Karl Pertsch and Karol Hausman and Keegan Go and Keerthana Gopalakrishnan and Ken Goldberg and Kendra Byrne and Kenneth Oslund and Kento Kawaharazuka and Kevin Zhang and Keyvan Majd and Krishan Rana and Krishnan Srinivasan and Lawrence Yunliang Chen and Lerrel Pinto and Liam Tan and Lionel Ott and Lisa Lee and Masayoshi Tomizuka and Maximilian Du and Michael Ahn and Mingtong Zhang and Mingyu Ding and Mohan Kumar Sriram"
        },
        {
            "comment": "The code snippet contains a list of authors' names separated by 'and', possibly used to acknowledge contributions or credits in a research paper, project documentation, or a similar context.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/README.md\":95-95",
            "content": "a and Mohit Sharma and Moo Jin Kim and Naoaki Kanazawa and Nicklas Hansen and Nicolas Heess and Nikhil J Joshi and Niko Suenderhauf and Norman Di Palo and Nur Muhammad Mahi Shafiullah and Oier Mees and Oliver Kroemer and Pannag R Sanketi and Paul Wohlhart and Peng Xu and Pierre Sermanet and Priya Sundaresan and Quan Vuong and Rafael Rafailov and Ran Tian and Ria Doshi and Roberto Mart\u00edn-Mart\u00edn and Russell Mendonca and Rutav Shah and Ryan Hoque and Ryan Julian and Samuel Bustamante and Sean Kirmani and Sergey Levine and Sherry Moore and Shikhar Bahl and Shivin Dass and Shuran Song and Sichun Xu and Siddhant Haldar and Simeon Adebola and Simon Guist and Soroush Nasiriany and Stefan Schaal and Stefan Welker and Stephen Tian and Sudeep Dasari and Suneel Belkhale and Takayuki Osa and Tatsuya Harada and Tatsuya Matsushima and Ted Xiao and Tianhe Yu and Tianli Ding and Todor Davchev and Tony Z. Zhao and Travis Armstrong and Trevor Darrell and Vidhi Jain and Vincent Vanhoucke and Wei Zhan and "
        },
        {
            "comment": "Integrating EfficientNetFilm into RTX-1, creating training script for basic cross entropy in first rt-1, using RTX-2 dataset from huggingface, and checking the project board for more tasks.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/README.md\":95-105",
            "content": "Wenxuan Zhou and Wolfram Burgard and Xi Chen and Xiaolong Wang and Xinghao Zhu and Xuanlin Li and Yao Lu and Yevgen Chebotar and Yifan Zhou and Yifeng Zhu and Ying Xu and Yixuan Wang and Yonatan Bisk and Yoonyoung Cho and Youngwoon Lee and Yuchen Cui and Yueh-hua Wu and Yujin Tang and Yuke Zhu and Yunzhu Li and Yusuke Iwasawa and Yutaka Matsuo and Zhuo Xu and Zichen Jeff Cui},\nhowpublished  = {\\url{https://arxiv.org/abs/2310.08864}},\nyear = {2023},\n}\n```\n# Todo\n- Integrate EfficientNetFilm with RTX-1\n- Create training script for RTX-1 by unrolling observations and do basic cross entropy in first rt-1\n- Use RTX-2 dataset on huggingface\n- [Check out the project board for more tasks](https://github.com/users/kyegomez/projects/10/views/1)"
        }
    ]
}