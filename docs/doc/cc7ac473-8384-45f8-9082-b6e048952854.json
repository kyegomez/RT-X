{
    "summary": "RTX2 is a customizable transformer architecture with ViT encoder and Transformer decoder, processing image-text inputs and offering attention options for output generation. The code defines a class RTX2 inherited from another class, initializing the encoder and decoder components and using an autoregressive wrapper for token generation.",
    "details": [
        {
            "comment": "RTX2 is a transformer architecture that uses ViT encoder and Transformer decoder. It has various parameters like image size, patch size, dimensions, depths, heads, number of tokens, etc. for customization.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx2.py\":0-29",
            "content": "import torch\nfrom zeta.structs import (\n    AutoregressiveWrapper,\n    Decoder,\n    Encoder,\n    Transformer,\n    ViTransformerWrapper,\n)\nclass RTX2(torch.nn.Module):\n    \"\"\"\n    RTX2 is a transformer architecture that uses a ViT encoder and a transformer decoder.\n    Args:\n        image_size (int): Size of the image.\n        patch_size (int): Size of the patch.\n        encoder_dim (int): Dimension of the encoder.\n        encoder_depth (int): Depth of the encoder.\n        encoder_heads (int): Number of heads in the encoder.\n        num_tokens (int): Number of tokens.\n        max_seq_len (int): Maximum sequence length.\n        decoder_dim (int): Dimension of the decoder.\n        decoder_depth (int): Depth of the decoder.\n        decoder_heads (int): Number of heads in the decoder.\n        alibi_num_heads (int): Number of heads in the alibi attention.\n        attn_kv_heads (int): Number of heads in the attention key-value projection.\n        use_abs_pos_emb (bool): Whether to use absolute positional embeddings.\n        cross_attend (bool): Whether to cross attend in the decoder."
        },
        {
            "comment": "This code defines a class called \"RTX2\" with several parameters and attributes. The model takes image and text inputs, processes them using transformers, and returns output. It has options for alibi attention, rotary positional embeddings, attention flash, and query/key normalization in the attention layer.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx2.py\":30-66",
            "content": "        alibi_pos_bias (bool): Whether to use positional bias in the alibi attention.\n        rotary_xpos (bool): Whether to use rotary positional embeddings.\n        attn_flash (bool): Whether to use attention flash.\n        qk_norm (bool): Whether to normalize the query and key in the attention layer.\n    Returns:\n            torch.Tensor: The output of the model.\n    Usage:\n            >>> img = torch.randn(1, 3, 256, 256)\n            >>> text = torch.randint(0, 20000, (1, 1024))\n            >>> model = RTX2()\n            >>> output = model(img, text)\n            >>> print(output)\n    \"\"\"\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=32,\n        encoder_dim=512,\n        encoder_depth=6,\n        encoder_heads=8,\n        num_tokens=20000,\n        max_seq_len=1024,\n        decoder_dim=512,\n        decoder_depth=6,\n        decoder_heads=8,\n        alibi_num_heads=4,\n        attn_kv_heads=2,\n        use_abs_pos_emb=False,\n        cross_attend=True,\n        alibi_pos_bias=True,\n        rotary_xpos=True,"
        },
        {
            "comment": "This code defines a class RTX2 that inherits from another class. It initializes the encoder and decoder components of the model using ViTransformerWrapper and Transformer classes, respectively. The encoder's architecture is specified by passing arguments to the Encoder class, while the decoder's architecture takes additional parameters for cross-attention, positional bias, rotary embedding, attention head configurations, and optional attn_flash and qk_norm features.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx2.py\":67-100",
            "content": "        attn_flash=True,\n        qk_norm=True,\n        *args,\n        **kwargs,\n    ):\n        super(RTX2, self).__init__()\n        # vit architecture\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim,\n                depth=encoder_depth,\n                heads=encoder_heads,\n            ),\n        )\n        # palm model architecture\n        self.decoder = Transformer(\n            num_tokens=num_tokens,\n            max_seq_len=max_seq_len,\n            use_abs_pos_emb=use_abs_pos_emb,\n            attn_layers=Decoder(\n                dim=decoder_dim,\n                depth=decoder_depth,\n                heads=decoder_heads,\n                cross_attend=cross_attend,\n                alibi_pos_bias=alibi_pos_bias,\n                alibi_num_heads=alibi_num_heads,\n                rotary_xpos=rotary_xpos,\n                attn_kv_heads=attn_kv_heads,\n                attn_flash=attn_flash,\n                qk_norm=qk_norm,"
        },
        {
            "comment": "This code defines a class with an initializer and a forward method. The initializer sets up the autoregressive wrapper for the decoder, allowing token generation. The forward method encodes the image using the encoder and passes the context to the decoder for prediction.",
            "location": "\"/media/root/Prima/works/RT-X/docs/src/rtx/rtx2.py\":101-114",
            "content": "            ),\n        )\n        # autoregressive wrapper to enable generation of tokens\n        self.decoder = AutoregressiveWrapper(self.decoder)\n    def forward(self, img: torch.Tensor, text: torch.Tensor):\n        \"\"\"Forward pass of the model.\"\"\"\n        try:\n            encoded = self.encoder(img, return_embeddings=True)\n            return self.decoder(text, context=encoded)\n        except Exception as error:\n            print(f\"Failed in forward method: {error}\")\n            raise"
        }
    ]
}