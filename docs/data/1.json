{
    "100": {
        "file_id": 10,
        "content": "        return x\n# Robotic Transformer\nclass RT1Config:\n    def __init__(\n        self,\n        num_actions=11,\n        action_bins=256,\n        depth=6,\n        heads=8,\n        dim_head=64,\n        token_learner_ff_mult=2,\n        token_learner_num_layers=2,\n        token_learner_num_output_tokens=8,\n        cond_drop_prob=0.2,\n        use_attn_conditioner=False,\n    ):\n        \"\"\"Configuration class to store the configuration of a `RT1`.\n        Args:\n            num_actions (int): Number of actions for the classification task\n            action_bins (int): Number of bins for each action\n            depth (int): Number of transformer blocks\n            heads (int): Number of heads for the transformer\n            dim_head (int): Dimension of the head\n            token_learner_ff_mult (int): Multiplier for the token learner\n            token_learner_num_layers (int): Number of layers for the token learner\n            token_learner_num_output_tokens (int): Number of output tokens for the token learner\n            cond_drop_prob (float): Dropout probability",
        "type": "code",
        "location": "/rtx/rtx1.py:738-769"
    },
    "101": {
        "file_id": 10,
        "content": "This code defines a configuration class, RT1Config, for the `RT1` model. It contains parameters such as number of actions, action bins, transformer block depth, and more. These settings can be customized to control the behavior and performance of the `RT1` model.",
        "type": "comment"
    },
    "102": {
        "file_id": 10,
        "content": "            use_attn_conditioner (bool): Whether to use the attention conditioner\n        \"\"\"\n        self.num_actions = num_actions\n        self.action_bins = action_bins\n        self.depth = depth\n        self.heads = heads\n        self.dim_head = dim_head\n        self.token_learner_ff_mult = token_learner_ff_mult\n        self.token_learner_num_layers = token_learner_num_layers\n        self.token_learner_num_output_tokens = (\n            token_learner_num_output_tokens\n        )\n        self.cond_drop_prob = cond_drop_prob\n        self.use_attn_conditioner = use_attn_conditioner\n@beartype\nclass RT1(nn.Module):\n    def __init__(\n        self,\n        config: RT1Config,\n        vit: FilmMaxVit,\n        conditioner_kwargs: dict = dict(),\n    ):\n        super().__init__()\n        self.vit = vit\n        self.num_vit_stages = len(vit.cond_hidden_dims)\n        film_layer = (\n            FilmAttentionTextConditioner\n            if config.use_attn_conditioner\n            else FilmTextConditioner\n        )\n        self.conditioner = film_layer(",
        "type": "code",
        "location": "/rtx/rtx1.py:770-804"
    },
    "103": {
        "file_id": 10,
        "content": "The code defines a class called RT1 that takes in a configuration, a Vision-Language model (vit), and optional conditioner kwargs. The class initializes the vit module and sets the number of vit stages based on the condition hidden dimensions. It also uses a FilmAttentionTextConditioner if use_attn_conditioner is set to True, otherwise it uses FilmTextConditioner.",
        "type": "comment"
    },
    "104": {
        "file_id": 10,
        "content": "            hidden_dims=(\n                *tuple(vit.cond_hidden_dims),\n                *((vit.embed_dim,) * config.depth * 2),\n            ),\n            hiddens_channel_first=(\n                *((True,) * self.num_vit_stages),\n                *((False,) * config.depth * 2),\n            ),\n            cond_drop_prob=config.cond_drop_prob,\n            **conditioner_kwargs,\n        )\n        self.token_learner = TokenLearner(\n            dim=vit.embed_dim,\n            ff_mult=config.token_learner_ff_mult,\n            num_output_tokens=config.token_learner_num_output_tokens,\n            num_layers=config.token_learner_num_layers,\n        )\n        self.num_learned_tokens = (\n            config.token_learner_num_output_tokens\n        )\n        self.transformer_depth = config.depth\n        self.transformer = Transformer(\n            dim=vit.embed_dim,\n            dim_head=config.dim_head,\n            heads=config.heads,\n            depth=config.depth,\n        )\n        self.cond_drop_prob = config.cond_drop_prob\n        self.to_logits = nn.Sequential(",
        "type": "code",
        "location": "/rtx/rtx1.py:805-839"
    },
    "105": {
        "file_id": 10,
        "content": "This code initializes a conditional transformer model. It sets the hidden dimensions, hiddens_channel_first (alternating True/False), and cond_drop_prob from config values. The TokenLearner is initialized with specified dimensions and configuration options. The number of learned tokens, transformer depth, and cond_drop_prob are also set. Lastly, a Transformer object is created using the provided dimensions, dim_head, heads, and depth.",
        "type": "comment"
    },
    "106": {
        "file_id": 10,
        "content": "            LayerNorm(vit.embed_dim),\n            nn.Linear(\n                vit.embed_dim, config.num_actions * config.action_bins\n            ),\n            Rearrange(\"... (a b) -> ... a b\", b=config.action_bins),\n        )\n    @classifier_free_guidance\n    def forward(\n        self,\n        video,\n        texts: Optional[List[str]] = None,\n        cond_drop_prob=0.0,\n    ):\n        depth = self.transformer_depth\n        cond_drop_prob = default(cond_drop_prob, self.cond_drop_prob)\n        frames, device = video.shape[2], video.device\n        cond_fns = self.conditioner(\n            texts,\n            cond_drop_prob=cond_drop_prob,\n            repeat_batch=(\n                *((frames,) * self.num_vit_stages),\n                *((1,) * self.transformer_depth * 2),\n            ),\n        )\n        vit_cond_fns, transformer_cond_fns = (\n            cond_fns[: -(depth * 2)],\n            cond_fns[-(depth * 2) :],\n        )\n        video = rearrange(video, \"b c f h w -> b f c h w\")\n        images, packed_shape = pack_one(video, \"* c h w\")",
        "type": "code",
        "location": "/rtx/rtx1.py:840-874"
    },
    "107": {
        "file_id": 10,
        "content": "This code defines a forward pass for a neural network that takes in video data and optional text inputs. The video passes through a series of transformations, including LayerNorm and linear operations, before being rearranged. The conditioner function generates conditional functions for the Vision Transformer (VIT) stages and the Transformer depth. These conditional functions are then used within the forward pass to guide the processing of the video data based on the text inputs.",
        "type": "comment"
    },
    "108": {
        "file_id": 10,
        "content": "        tokens = self.vit(\n            images,\n            texts=texts,\n            cond_fns=vit_cond_fns,\n            cond_drop_prob=cond_drop_prob,\n            return_embeddings=True,\n        )\n        tokens = unpack_one(tokens, packed_shape, \"* c h w\")\n        learned_tokens = self.token_learner(tokens)\n        learned_tokens = rearrange(\n            learned_tokens, \"b f c n -> b (f n) c\"\n        )\n        # causal attention mask\n        attn_mask = torch.ones(\n            (frames, frames), dtype=torch.bool, device=device\n        ).triu(1)\n        attn_mask = repeat(\n            attn_mask,\n            \"i j -> (i r1) (j r2)\",\n            r1=self.num_learned_tokens,\n            r2=self.num_learned_tokens,\n        )\n        # sinusoidal positional embedding\n        pos_emb = posemb_sincos_1d(\n            frames,\n            learned_tokens.shape[-1],\n            dtype=learned_tokens.dtype,\n            device=learned_tokens.device,\n        )\n        learned_tokens = learned_tokens + repeat(\n            pos_emb, \"n d -> (n r) d\", r=self.num_learned_tokens",
        "type": "code",
        "location": "/rtx/rtx1.py:876-913"
    },
    "109": {
        "file_id": 10,
        "content": "The code is performing token learning and feature extraction using a Vision-Language Model (VLM) called \"vit\". It then applies rearrangement, attention masking, and sinusoidal positional embedding to the learned tokens for further processing.",
        "type": "comment"
    },
    "110": {
        "file_id": 10,
        "content": "        )\n        # attention\n        attended_tokens = self.transformer(\n            learned_tokens,\n            cond_fns=transformer_cond_fns,\n            attn_mask=~attn_mask,\n        )\n        pooled = reduce(\n            attended_tokens, \"b (f n) d -> b f d\", \"mean\", f=frames\n        )\n        logits = self.to_logits(pooled)\n        return logits\nclass RTX1(nn.Module):\n    \"\"\"\n    A class for real-time video processing using Vision Transformers (ViT) and Reinforcement Learning (RT1) models.\n    ...\n    Attributes\n    ----------\n    vit : FilmMaxVit\n        a Vision Transformer model\n    model : RT1\n        a reinforcement learning model\n    Methods\n    -------\n    train(video, instructions):\n        Computes the logits for the given video and instructions using the RT1 model in training mode.\n    eval(video, instructions, cond_scale=1.0):\n        Computes the logits for the given video and instructions using the RT1 model in evaluation mode.\n    \"\"\"\n    def __init__(\n        self,\n        rt1_config: RT1Config = None,",
        "type": "code",
        "location": "/rtx/rtx1.py:914-955"
    },
    "111": {
        "file_id": 10,
        "content": "This code defines a class for real-time video processing using Vision Transformers (ViT) and Reinforcement Learning (RT1) models. It has train and eval methods that compute logits for videos and instructions using the RT1 model in respective modes.",
        "type": "comment"
    },
    "112": {
        "file_id": 10,
        "content": "        vit_config: FilmViTConfig = None,\n    ):\n        \"\"\"\n        Constructs all the necessary attributes for the RTX1 object.\n        Parameters\n        ----------\n        rt1_config : RT1Config, optional\n            a configuration object for the RT1 model (default is None)\n        vit_config : FilmViTConfig, optional\n            a configuration object for the ViT model (default is None)\n        Example:\n        import torch\n        from rtx import RTX1\n        model = RTX1()\n        video = torch.randn(2, 3, 6, 224, 224)\n        instructions = [\"bring me that apple sitting on the table\", \"please pass the butter\"]\n        # compute the train logits\n        train_logits = model.train(video, instructions)\n        # set the model to evaluation mode\n        model.model.eval()\n        # compute the eval logits with a conditional scale of 3\n        eval_logits = model.run(video, instructions, cond_scale=3.0)\n        print(eval_logits.shape)\n        \"\"\"\n        super().__init__()\n        if rt1_config is None:\n            rt1_config = RT1Config()",
        "type": "code",
        "location": "/rtx/rtx1.py:956-993"
    },
    "113": {
        "file_id": 10,
        "content": "The code is a constructor for the RTX1 object, which initializes attributes from optional configuration objects for RT1 and ViT models. It sets default configurations if not specified and can be used to train or evaluate the model based on inputs and conditional scale.",
        "type": "comment"
    },
    "114": {
        "file_id": 10,
        "content": "        if vit_config is None:\n            vit_config = FilmViTConfig()\n        self.vit = FilmMaxVit(vit_config)\n        self.model = RT1(\n            config=rt1_config,\n            vit=self.vit,\n        )\n    def train(self, video, instructions):\n        \"\"\"\n        Computes the logits for the given video and instructions using the RT1 model in training mode.\n        Parameters\n        ----------\n        video : torch.Tensor\n            a tensor containing the video data\n        instructions : torch.Tensor\n            a tensor containing the instructions\n        Returns\n        -------\n        torch.Tensor\n            a tensor containing the computed logits\n        \"\"\"\n        try:\n            train_logits = self.model(video, instructions)\n            return train_logits\n        except Exception as e:\n            raise RuntimeError(\"Error in training: {}\".format(e))\n    def run(self, video, instructions, cond_scale=1.0):\n        \"\"\"\n        Computes the logits for the given video and instructions using the RT1 model in evaluation mode.",
        "type": "code",
        "location": "/rtx/rtx1.py:994-1028"
    },
    "115": {
        "file_id": 10,
        "content": "This code snippet is from a file named \"rtx1.py\". It checks if vit_config is None and initializes it as FilmViTConfig() if so. Then, it creates an instance of FilmMaxVit with the vit_config. The RT1 model is then initialized with rt1_config and the previously created FilmMaxVit instance. The train method computes logits for video and instructions using the RT1 model in training mode. If an exception occurs during training, a RuntimeError is raised. The run method computes logits for video and instructions using the RT1 model in evaluation mode.",
        "type": "comment"
    },
    "116": {
        "file_id": 10,
        "content": "        Parameters\n        ----------\n        video : torch.Tensor\n            a tensor containing the video data\n        instructions : torch.Tensor\n            a tensor containing the instructions\n        cond_scale : float, optional\n            a scale factor for the conditional scaling (default is 1.0)\n        Returns\n        -------\n        torch.Tensor\n            a tensor containing the computed logits\n        \"\"\"\n        try:\n            self.model.eval()\n            # shape => 2, 3, 6, 224, 224\n            eval_logits = self.model(\n                video, instructions, cond_scale=cond_scale\n            )\n            return eval_logits\n        except Exception as e:\n            raise RuntimeError(\"Error in evaluation: {}\".format(e))",
        "type": "code",
        "location": "/rtx/rtx1.py:1030-1054"
    },
    "117": {
        "file_id": 10,
        "content": "This function takes video and instruction data as input, along with an optional conditional scale factor. It applies the model's evaluation mode and returns a tensor of computed logits after processing the given inputs through the model. In case of any exception during this process, it raises a RuntimeError with the specific error message.",
        "type": "comment"
    },
    "118": {
        "file_id": 11,
        "content": "/rtx/rtx2.py",
        "type": "filepath"
    },
    "119": {
        "file_id": 11,
        "content": "RTX2 is a customizable transformer architecture with ViT encoder and Transformer decoder, processing image-text inputs and offering attention options for output generation. The code defines a class RTX2 inherited from another class, initializing the encoder and decoder components and using an autoregressive wrapper for token generation.",
        "type": "summary"
    },
    "120": {
        "file_id": 11,
        "content": "import torch\nfrom zeta.structs import (\n    AutoregressiveWrapper,\n    Decoder,\n    Encoder,\n    Transformer,\n    ViTransformerWrapper,\n)\nclass RTX2(torch.nn.Module):\n    \"\"\"\n    RTX2 is a transformer architecture that uses a ViT encoder and a transformer decoder.\n    Args:\n        image_size (int): Size of the image.\n        patch_size (int): Size of the patch.\n        encoder_dim (int): Dimension of the encoder.\n        encoder_depth (int): Depth of the encoder.\n        encoder_heads (int): Number of heads in the encoder.\n        num_tokens (int): Number of tokens.\n        max_seq_len (int): Maximum sequence length.\n        decoder_dim (int): Dimension of the decoder.\n        decoder_depth (int): Depth of the decoder.\n        decoder_heads (int): Number of heads in the decoder.\n        alibi_num_heads (int): Number of heads in the alibi attention.\n        attn_kv_heads (int): Number of heads in the attention key-value projection.\n        use_abs_pos_emb (bool): Whether to use absolute positional embeddings.\n        cross_attend (bool): Whether to cross attend in the decoder.",
        "type": "code",
        "location": "/rtx/rtx2.py:1-30"
    },
    "121": {
        "file_id": 11,
        "content": "RTX2 is a transformer architecture that uses ViT encoder and Transformer decoder. It has various parameters like image size, patch size, dimensions, depths, heads, number of tokens, etc. for customization.",
        "type": "comment"
    },
    "122": {
        "file_id": 11,
        "content": "        alibi_pos_bias (bool): Whether to use positional bias in the alibi attention.\n        rotary_xpos (bool): Whether to use rotary positional embeddings.\n        attn_flash (bool): Whether to use attention flash.\n        qk_norm (bool): Whether to normalize the query and key in the attention layer.\n    Returns:\n            torch.Tensor: The output of the model.\n    Usage:\n            >>> img = torch.randn(1, 3, 256, 256)\n            >>> text = torch.randint(0, 20000, (1, 1024))\n            >>> model = RTX2()\n            >>> output = model(img, text)\n            >>> print(output)\n    \"\"\"\n    def __init__(\n        self,\n        image_size=256,\n        patch_size=32,\n        encoder_dim=512,\n        encoder_depth=6,\n        encoder_heads=8,\n        num_tokens=20000,\n        max_seq_len=1024,\n        decoder_dim=512,\n        decoder_depth=6,\n        decoder_heads=8,\n        alibi_num_heads=4,\n        attn_kv_heads=2,\n        use_abs_pos_emb=False,\n        cross_attend=True,\n        alibi_pos_bias=True,\n        rotary_xpos=True,",
        "type": "code",
        "location": "/rtx/rtx2.py:31-67"
    },
    "123": {
        "file_id": 11,
        "content": "This code defines a class called \"RTX2\" with several parameters and attributes. The model takes image and text inputs, processes them using transformers, and returns output. It has options for alibi attention, rotary positional embeddings, attention flash, and query/key normalization in the attention layer.",
        "type": "comment"
    },
    "124": {
        "file_id": 11,
        "content": "        attn_flash=True,\n        qk_norm=True,\n        *args,\n        **kwargs,\n    ):\n        super(RTX2, self).__init__()\n        # vit architecture\n        self.encoder = ViTransformerWrapper(\n            image_size=image_size,\n            patch_size=patch_size,\n            attn_layers=Encoder(\n                dim=encoder_dim,\n                depth=encoder_depth,\n                heads=encoder_heads,\n            ),\n        )\n        # palm model architecture\n        self.decoder = Transformer(\n            num_tokens=num_tokens,\n            max_seq_len=max_seq_len,\n            use_abs_pos_emb=use_abs_pos_emb,\n            attn_layers=Decoder(\n                dim=decoder_dim,\n                depth=decoder_depth,\n                heads=decoder_heads,\n                cross_attend=cross_attend,\n                alibi_pos_bias=alibi_pos_bias,\n                alibi_num_heads=alibi_num_heads,\n                rotary_xpos=rotary_xpos,\n                attn_kv_heads=attn_kv_heads,\n                attn_flash=attn_flash,\n                qk_norm=qk_norm,",
        "type": "code",
        "location": "/rtx/rtx2.py:68-101"
    },
    "125": {
        "file_id": 11,
        "content": "This code defines a class RTX2 that inherits from another class. It initializes the encoder and decoder components of the model using ViTransformerWrapper and Transformer classes, respectively. The encoder's architecture is specified by passing arguments to the Encoder class, while the decoder's architecture takes additional parameters for cross-attention, positional bias, rotary embedding, attention head configurations, and optional attn_flash and qk_norm features.",
        "type": "comment"
    },
    "126": {
        "file_id": 11,
        "content": "            ),\n        )\n        # autoregressive wrapper to enable generation of tokens\n        self.decoder = AutoregressiveWrapper(self.decoder)\n    def forward(self, img: torch.Tensor, text: torch.Tensor):\n        \"\"\"Forward pass of the model.\"\"\"\n        try:\n            encoded = self.encoder(img, return_embeddings=True)\n            return self.decoder(text, context=encoded)\n        except Exception as error:\n            print(f\"Failed in forward method: {error}\")\n            raise",
        "type": "code",
        "location": "/rtx/rtx2.py:102-115"
    },
    "127": {
        "file_id": 11,
        "content": "This code defines a class with an initializer and a forward method. The initializer sets up the autoregressive wrapper for the decoder, allowing token generation. The forward method encodes the image using the encoder and passes the context to the decoder for prediction.",
        "type": "comment"
    },
    "128": {
        "file_id": 12,
        "content": "/run_example.py",
        "type": "filepath"
    },
    "129": {
        "file_id": 12,
        "content": "This code imports various example functions, defines a dictionary of example names and their corresponding functions, and sets the default model type as \"rtx1\". It then creates an argument parser for command line input, allowing the user to specify a different example from the available options. Finally, it uses the provided arguments to run the selected example function.",
        "type": "summary"
    },
    "130": {
        "file_id": 12,
        "content": "import argparse\nfrom examples import (\n    rtx1_example,\n    rtx1_pretrained_example,\n    rtx2_example,\n)\nEXAMPLES = {\n    \"rtx1\": rtx1_example,\n    \"rtx1-pretrained\": rtx1_pretrained_example,\n    \"rtx2\": rtx2_example,\n}\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-m\",\n        \"--model_type\",\n        type=str,\n        default=\"rtx1\",\n        choices=EXAMPLES.keys(),\n        help=\"Example to choose from\",\n    )\n    args = parser.parse_args()\n    EXAMPLES[args.model_type].run()",
        "type": "code",
        "location": "/run_example.py:1-25"
    },
    "131": {
        "file_id": 12,
        "content": "This code imports various example functions, defines a dictionary of example names and their corresponding functions, and sets the default model type as \"rtx1\". It then creates an argument parser for command line input, allowing the user to specify a different example from the available options. Finally, it uses the provided arguments to run the selected example function.",
        "type": "comment"
    },
    "132": {
        "file_id": 13,
        "content": "/tests.py",
        "type": "filepath"
    },
    "133": {
        "file_id": 13,
        "content": "The code tests EfficientNetFilm model's features and exception handling, using two test cases with valid and invalid inputs. It verifies forward pass, checks invalid inputs, and tests configurations with pytest's raises context manager.",
        "type": "summary"
    },
    "134": {
        "file_id": 13,
        "content": "import pytest\nimport torch\nfrom PIL import Image\nfrom zeta.structs import (\n    AutoregressiveWrapper,\n    ViTransformerWrapper,\n)\nfrom rtx.efficient_net import EfficientNetFilm\nfrom rtx.rtx1 import RT1, RTX1, MaxViT\nfrom rtx.rtx2 import RTX2\n########################### EfficientNetFilm ###########################\nimg = \"img.jpeg\"\n# Fixture to create an instance of the EfficientNetFilm class\n@pytest.fixture\ndef efficientnet_model():\n    model = EfficientNetFilm(\"efficientnet-b0\", 10)\n    return model\n# Test case to check if EfficientNetFilm initializes correctly\ndef test_efficientnet_init(efficientnet_model):\n    assert efficientnet_model is not None\n# Test case to check if EfficientNetFilm processes an image correctly\ndef test_efficientnet_process_image(efficientnet_model):\n    # Load a sample image\n    image_path = img\n    Image.open(image_path)\n    # Process the image using the model\n    features = efficientnet_model(image_path)\n    # Check if the output features are of the correct shape\n    assert isinstance(features, torch.Tensor)",
        "type": "code",
        "location": "/tests.py:1-40"
    },
    "135": {
        "file_id": 13,
        "content": "This code defines a test fixture for EfficientNetFilm and two test cases to verify its initialization and image processing capabilities. The model is initialized with the \"efficientnet-b0\" architecture and 10 output channels. It loads an image, processes it using the model, and checks if the features are of the correct shape (a torch.Tensor).",
        "type": "comment"
    },
    "136": {
        "file_id": 13,
        "content": "    assert features.shape == (1, efficientnet_model.num_features)\n# Test case to check if EfficientNetFilm handles image resizing correctly\ndef test_efficientnet_image_resize(efficientnet_model):\n    # Load a sample image\n    image_path = img\n    image = Image.open(image_path)\n    # Process the image using the model\n    efficientnet_model(image_path)\n    # Check if the input image was resized to the specified size\n    assert image.size == (\n        efficientnet_model.resize,\n        efficientnet_model.resize,\n    )\n# Test case to check if EfficientNetFilm handles model loading correctly\ndef test_efficientnet_model_loading(efficientnet_model):\n    # Check if the model was loaded successfully\n    assert efficientnet_model.model is not None\n# Test case to check if EfficientNetFilm handles image transformations correctly\ndef test_efficientnet_image_transformations(efficientnet_model):\n    # Load a sample image\n    image_path = img\n    Image.open(image_path)\n    # Process the image using the model\n    features = efficientnet_model(image_path)",
        "type": "code",
        "location": "/tests.py:41-73"
    },
    "137": {
        "file_id": 13,
        "content": "The code contains three test cases: 1) image resizing, 2) model loading, and 3) image transformations in the EfficientNetFilm. It first checks if the input image is resized correctly using the efficientnet_model.resize parameter. Next, it verifies if the model was loaded successfully. Finally, it tests if the image processing features were extracted correctly by the model.",
        "type": "comment"
    },
    "138": {
        "file_id": 13,
        "content": "    # Check if image transformations were applied correctly\n    assert torch.max(features).item() <= 1.0\n    assert torch.min(features).item() >= -1.0\n# Test case to check if EfficientNetFilm handles the number of classes correctly\ndef test_efficientnet_num_classes(efficientnet_model):\n    # Check if the number of classes is set correctly\n    assert efficientnet_model.num_classes == 10\n# Test case to check if EfficientNetFilm handles missing image file correctly\ndef test_efficientnet_missing_image(efficientnet_model):\n    with pytest.raises(FileNotFoundError):\n        efficientnet_model(\"non_existent_image.jpg\")\n# Test case to check if EfficientNetFilm handles incorrect image file format correctly\ndef test_efficientnet_incorrect_image_format(efficientnet_model):\n    with pytest.raises(ValueError):\n        efficientnet_model(\"sample_image.txt\")\n# Test case to check if EfficientNetFilm handles model selection correctly\ndef test_efficientnet_model_selection():\n    # Check if different EfficientNet models can be selected",
        "type": "code",
        "location": "/tests.py:75-100"
    },
    "139": {
        "file_id": 13,
        "content": "The code includes various test cases to validate the functionalities of EfficientNetFilm. It checks if image transformations are applied correctly, verifies the number of classes is set appropriately, handles missing and incorrectly formatted images, and ensures that different models can be selected.",
        "type": "comment"
    },
    "140": {
        "file_id": 13,
        "content": "    model_names = [\n        \"efficientnet-b0\",\n        \"efficientnet-b1\",\n        \"efficientnet-b2\",\n    ]\n    for model_name in model_names:\n        model = EfficientNetFilm(model_name, 10)\n        assert model is not None\n        assert model.model is not None\n# Test case to check if EfficientNetFilm handles invalid model name correctly\ndef test_efficientnet_invalid_model_name():\n    with pytest.raises(ValueError):\n        EfficientNetFilm(\"invalid_model\", 10)\n# Test case to check if EfficientNetFilm handles invalid number of classes correctly\ndef test_efficientnet_invalid_num_classes():\n    with pytest.raises(ValueError):\n        EfficientNetFilm(\"efficientnet-b0\", -10)\n# Test case to check if EfficientNetFilm handles invalid resize size correctly\ndef test_efficientnet_invalid_resize_size():\n    with pytest.raises(ValueError):\n        EfficientNetFilm(\"efficientnet-b0\", 10, resize=-100)\n# Test case to check if EfficientNetFilm handles input image with incorrect channels correctly\ndef test_efficientnet_incorrect_image_channels(efficientnet_model):",
        "type": "code",
        "location": "/tests.py:101-131"
    },
    "141": {
        "file_id": 13,
        "content": "The code defines a list of model names for EfficientNetFilm and asserts that the model is not None and its associated model is also not None. Then, it includes four test cases to check if the EfficientNetFilm handles invalid inputs: invalid model name, invalid number of classes, invalid resize size, and incorrect image channels.",
        "type": "comment"
    },
    "142": {
        "file_id": 13,
        "content": "    # Create an image with incorrect number of channels (4 channels)\n    image = Image.new(\n        \"RGBA\",\n        (efficientnet_model.resize, efficientnet_model.resize),\n        (255, 0, 0, 255),\n    )\n    image_path = \"incorrect_channels_image.png\"\n    image.save(image_path)\n    with pytest.raises(ValueError):\n        efficientnet_model(image_path)\n# Test case to check if EfficientNetFilm handles input image with incorrect size correctly\ndef test_efficientnet_incorrect_image_size(efficientnet_model):\n    # Create an image with incorrect size (smaller than resize size)\n    image = Image.new(\n        \"RGB\",\n        (\n            efficientnet_model.resize - 1,\n            efficientnet_model.resize - 1,\n        ),\n        (255, 0, 0),\n    )\n    image_path = \"incorrect_size_image.jpg\"\n    image.save(image_path)\n    with pytest.raises(ValueError):\n        efficientnet_model(image_path)\n########################### RTX1 ###########################\n# Fixture to create an instance of the RTX1 class\n@pytest.fixture\ndef rtx1_model():",
        "type": "code",
        "location": "/tests.py:132-168"
    },
    "143": {
        "file_id": 13,
        "content": "This code tests the EfficientNetFilm model's ability to handle input images with incorrect channel, size, and file format. It creates an image with 4 channels (RGBA), smaller dimensions than expected, and an invalid file format before testing if the model correctly raises a ValueError.",
        "type": "comment"
    },
    "144": {
        "file_id": 13,
        "content": "    model = RTX1()\n    return model\n# Test case to check if RTX1 initializes correctly\ndef test_rtx1_initialization(rtx1_model):\n    assert isinstance(rtx1_model, RTX1)\n    assert isinstance(rtx1_model.vit, MaxViT)\n    assert isinstance(rtx1_model.model, RT1)\n# Test case to check if RTX1 handles training with video and instructions correctly\ndef test_rtx1_train(rtx1_model):\n    video = torch.randn(2, 3, 6, 224, 224)\n    instructions = [\n        \"bring me that apple sitting on the table\",\n        \"please pass the butter\",\n    ]\n    train_logits = rtx1_model.train(video, instructions)\n    assert isinstance(train_logits, torch.Tensor)\n    assert train_logits.shape == (2, rtx1_model.num_actions)\n# Test case to check if RTX1 handles evaluation with video and instructions correctly\ndef test_rtx1_eval(rtx1_model):\n    video = torch.randn(2, 3, 6, 224, 224)\n    instructions = [\n        \"bring me that apple sitting on the table\",\n        \"please pass the butter\",\n    ]\n    eval_logits = rtx1_model.run(video, instructions, cond_scale=3.0)",
        "type": "code",
        "location": "/tests.py:169-202"
    },
    "145": {
        "file_id": 13,
        "content": "This code defines the RTX1 model and includes test cases for checking its initialization, training, and evaluation. It first initializes the RTX1 model and then tests if it is correctly initialized with MaxViT and RT1 components. Next, it tests if the model can handle video and instruction inputs during training, ensuring the return logits are of correct shape. Finally, it checks if the model can process video and instruction inputs during evaluation while specifying a cond_scale parameter.",
        "type": "comment"
    },
    "146": {
        "file_id": 13,
        "content": "    assert isinstance(eval_logits, torch.Tensor)\n    assert eval_logits.shape == (2, rtx1_model.num_actions)\n# Test case to check if RTX1 raises an error when training with invalid inputs\ndef test_rtx1_train_with_invalid_inputs(rtx1_model):\n    with pytest.raises(RuntimeError):\n        video = torch.randn(2, 3, 6, 224, 224)\n        instructions = [\n            \"bring me that apple sitting on the table\",\n            \"please pass the butter\",\n        ]\n        # Intentionally set an invalid shape for instructions\n        instructions = instructions[\n            :1\n        ]  # Instructions shape should be (2,)\n        rtx1_model.train(video, instructions)\n# Test case to check if RTX1 raises an error when evaluating with invalid inputs\ndef test_rtx1_eval_with_invalid_inputs(rtx1_model):\n    with pytest.raises(RuntimeError):\n        video = torch.randn(2, 3, 6, 224, 224)\n        instructions = [\n            \"bring me that apple sitting on the table\",\n            \"please pass the butter\",\n        ]\n        # Intentionally set an invalid shape for video",
        "type": "code",
        "location": "/tests.py:204-231"
    },
    "147": {
        "file_id": 13,
        "content": "The code includes two test cases. The first one tests if the RTX1 model raises an error when trained with invalid inputs, while the second tests if it raises an error during evaluation with invalid inputs. Both tests involve intentionally setting invalid shapes for either instructions or video and then calling the corresponding model function.",
        "type": "comment"
    },
    "148": {
        "file_id": 13,
        "content": "        video = video[\n            :, :, :5\n        ]  # Video shape should be (2, 3, 6, 224, 224)\n        rtx1_model.run(video, instructions, cond_scale=3.0)\n# Test case to check if RTX1 handles conditional scaling correctly\ndef test_rtx1_conditional_scaling(rtx1_model):\n    video = torch.randn(2, 3, 6, 224, 224)\n    instructions = [\n        \"bring me that apple sitting on the table\",\n        \"please pass the butter\",\n    ]\n    eval_logits = rtx1_model.run(video, instructions, cond_scale=3.0)\n    eval_logits_without_scaling = rtx1_model.run(video, instructions)\n    # Check if the logits with and without scaling are different\n    assert not torch.allclose(\n        eval_logits, eval_logits_without_scaling\n    )\n# Test case to check if RTX1 handles model selection correctly\ndef test_rtx1_model_selection():\n    model_names = [\n        \"efficientnet-b0\",\n        \"efficientnet-b1\",\n        \"efficientnet-b2\",\n    ]\n    for model_name in model_names:\n        model = RTX1(model_name=model_name)\n        assert isinstance(model, RTX1)",
        "type": "code",
        "location": "/tests.py:232-264"
    },
    "149": {
        "file_id": 13,
        "content": "The code snippet tests the conditional scaling and model selection capabilities of the RTX1 model. It ensures that the logits with scaling are different from those without, and verifies that the model correctly identifies valid names for different models (efficientnet-b0, efficientnet-b1, efficientnet-b2).",
        "type": "comment"
    },
    "150": {
        "file_id": 13,
        "content": "# Test case to check if RTX1 raises an error for an invalid model name\ndef test_rtx1_invalid_model_name():\n    with pytest.raises(ValueError):\n        RTX1(model_name=\"invalid_model\")\n# Test case to check if RTX1 handles negative number of classes correctly\ndef test_rtx1_negative_num_classes():\n    with pytest.raises(ValueError):\n        RTX1(num_classes=-100)\n# Test case to check if RTX1 handles negative dimension correctly\ndef test_rtx1_negative_dimension():\n    with pytest.raises(ValueError):\n        RTX1(dim=-96)\n# Test case to check if RTX1 handles negative dimension of convolutional stem correctly\ndef test_rtx1_negative_dim_conv_stem():\n    with pytest.raises(ValueError):\n        RTX1(dim_conv_stem=-64)\n# Test case to check if RTX1 handles negative dimension of head for ViT correctly\ndef test_rtx1_negative_dim_head_vit():\n    with pytest.raises(ValueError):\n        RTX1(dim_head_vit=-32)\n# Test case to check if RTX1 handles negative depth of ViT correctly\ndef test_rtx1_negative_depth_vit():\n    with pytest.raises(ValueError):",
        "type": "code",
        "location": "/tests.py:267-299"
    },
    "151": {
        "file_id": 13,
        "content": "The code includes six test cases that check if the RTX1 class raises a ValueError for invalid inputs. The test cases cover scenarios of an incorrect model name, negative number of classes, negative dimension, negative dimension of convolutional stem, negative dimension of head for ViT, and negative depth of ViT.",
        "type": "comment"
    },
    "152": {
        "file_id": 13,
        "content": "        RTX1(depth_vit=(-2, 2, 5, 2))\n# Test case to check if RTX1 handles negative window size for ViT correctly\ndef test_rtx1_negative_window_size():\n    with pytest.raises(ValueError):\n        RTX1(window_size=-7)\n# Test case to check if RTX1 handles negative expansion rate for mbconv correctly\ndef test_rtx1_negative_mbconv_expansion_rate():\n    with pytest.raises(ValueError):\n        RTX1(mbconv_expansion_rate=-4)\n# Test case to check if RTX1 handles negative shrinkage rate for mbconv correctly\ndef test_rtx1_negative_mbconv_shrinkage_rate():\n    with pytest.raises(ValueError):\n        RTX1(mbconv_shrinkage_rate=-0.25)\n# Test case to check if RTX1 handles negative dropout rate for ViT correctly\ndef test_rtx1_negative_dropout_vit():\n    with pytest.raises(ValueError):\n        RTX1(dropout_vit=-0.1)\n# Test case to check if RTX1 handles negative number of actions correctly\ndef test_rtx1_negative_num_actions():\n    with pytest.raises(ValueError):\n        RTX1(num_actions=-11)\n# Test case to check if RTX1 handles negative depth of RT1 correctly",
        "type": "code",
        "location": "/tests.py:300-333"
    },
    "153": {
        "file_id": 13,
        "content": "The code snippet includes five test cases to verify the exception handling of RTX1 for negative values. The test cases check if RTX1 raises a ValueError when given negative window size for ViT, negative expansion rate and shrinkage rate for mbconv, negative dropout rate for Vit, negative number of actions, and negative depth of RT1.",
        "type": "comment"
    },
    "154": {
        "file_id": 13,
        "content": "def test_rtx1_negative_depth_rt1():\n    with pytest.raises(ValueError):\n        RTX1(depth_rt1=-6)\n# Test case to check if RTX1 handles negative number of heads for RT1 correctly\ndef test_rtx1_negative_heads():\n    with pytest.raises(ValueError):\n        RTX1(heads=-8)\n# Test case to check if RTX1 handles negative dimension of head for RT1 correctly\ndef test_rtx1_negative_dim_head_rt1():\n    with pytest.raises(ValueError):\n        RTX1(dim_head_rt1=-64)\n# Test case to check if RTX1 handles negative conditional drop probability for RT1 correctly\ndef test_rtx1_negative_cond_drop_prob():\n    with pytest.raises(ValueError):\n        RTX1(cond_drop_prob=-0.2)\n########################### RTX2 ###########################\n# Fixture to create an instance of the RTX2 class\n@pytest.fixture\ndef rtx2_model():\n    model = RTX2()\n    return model\n# Test case to check if RTX2 initializes correctly\ndef test_rtx2_initialization(rtx2_model):\n    assert isinstance(rtx2_model, RTX2)\n    assert isinstance(rtx2_model.encoder, ViTransformerWrapper)",
        "type": "code",
        "location": "/tests.py:334-370"
    },
    "155": {
        "file_id": 13,
        "content": "These tests are designed to ensure that the RTX1 and RTX2 classes handle various types of negative inputs correctly. The code includes test cases for handling negative depth, number of heads, dimension of head, and conditional drop probability for RTX1, as well as an initialization test case for RTX2.",
        "type": "comment"
    },
    "156": {
        "file_id": 13,
        "content": "    assert isinstance(rtx2_model.decoder, AutoregressiveWrapper)\n# Test case to check if RTX2 handles forward pass with image and text correctly\ndef test_rtx2_forward_pass(rtx2_model):\n    img = torch.randn(1, 3, 256, 256)\n    text = torch.randint(0, 20000, (1, 1024))\n    output = rtx2_model(img, text)\n    assert isinstance(output, torch.Tensor)\n# Test case to check if RTX2 raises an error when forwarding with invalid inputs\ndef test_rtx2_forward_with_invalid_inputs(rtx2_model):\n    with pytest.raises(Exception):\n        img = torch.randn(1, 3, 256, 256)\n        text = torch.randn(\n            1, 1024, 512\n        )  # Invalid shape for text input\n        rtx2_model(img, text)\n# Test case to check if RTX2 handles various model configurations correctly\ndef test_rtx2_with_different_configs():\n    config_combinations = [\n        {\"encoder_depth\": 6, \"decoder_depth\": 6},\n        {\"encoder_depth\": 4, \"decoder_depth\": 8},\n        {\"encoder_heads\": 8, \"decoder_heads\": 8},\n        {\"encoder_dim\": 512, \"decoder_dim\": 768},",
        "type": "code",
        "location": "/tests.py:371-400"
    },
    "157": {
        "file_id": 13,
        "content": "This code tests the functionality of RTX2 model by asserting if it handles forward pass with image and text, checks for error when using invalid inputs, and verifies its behavior with different configurations.",
        "type": "comment"
    },
    "158": {
        "file_id": 13,
        "content": "    ]\n    for config in config_combinations:\n        model = RTX2(**config)\n        assert isinstance(model, RTX2)\n        assert (\n            model.encoder.attn_layers.depth == config[\"encoder_depth\"]\n        )\n        assert (\n            model.decoder.attn_layers.depth == config[\"decoder_depth\"]\n        )\n        if \"encoder_heads\" in config:\n            assert (\n                model.encoder.attn_layers.heads\n                == config[\"encoder_heads\"]\n            )\n        if \"decoder_heads\" in config:\n            assert (\n                model.decoder.attn_layers.heads\n                == config[\"decoder_heads\"]\n            )\n        if \"encoder_dim\" in config:\n            assert (\n                model.encoder.attn_layers.dim == config[\"encoder_dim\"]\n            )\n        if \"decoder_dim\" in config:\n            assert (\n                model.decoder.attn_layers.dim == config[\"decoder_dim\"]\n            )\n# Test case to check if RTX2 handles negative image size correctly\ndef test_rtx2_negative_image_size():\n    with pytest.raises(ValueError):",
        "type": "code",
        "location": "/tests.py:401-434"
    },
    "159": {
        "file_id": 13,
        "content": "Code tests RTX2 model configurations for correct depth, heads, and dimensions. It also includes a test case to ensure RTX2 handles negative image sizes correctly by raising a ValueError.",
        "type": "comment"
    },
    "160": {
        "file_id": 13,
        "content": "        RTX2(image_size=-256)\n# Test case to check if RTX2 handles negative patch size correctly\ndef test_rtx2_negative_patch_size():\n    with pytest.raises(ValueError):\n        RTX2(patch_size=-32)\n# Test case to check if RTX2 handles negative encoder dimension correctly\ndef test_rtx2_negative_encoder_dim():\n    with pytest.raises(ValueError):\n        RTX2(encoder_dim=-512)\n# Test case to check if RTX2 handles negative encoder depth correctly\ndef test_rtx2_negative_encoder_depth():\n    with pytest.raises(ValueError):\n        RTX2(encoder_depth=-6)\n# Test case to check if RTX2 handles negative decoder dimension correctly\ndef test_rtx2_negative_decoder_dim():\n    with pytest.raises(ValueError):\n        RTX2(decoder_dim=-512)\n# Test case to check if RTX2 handles negative decoder depth correctly\ndef test_rtx2_negative_decoder_depth():\n    with pytest.raises(ValueError):\n        RTX2(decoder_depth=-6)\n# Test case to check if RTX2 handles negative encoder heads correctly\ndef test_rtx2_negative_encoder_heads():\n    with pytest.raises(ValueError):",
        "type": "code",
        "location": "/tests.py:435-470"
    },
    "161": {
        "file_id": 13,
        "content": "The code contains several test cases for the RTX2 class. It checks if the RTX2 handles negative patch size, encoder dimension and depth, decoder dimension and depth, and encoder heads correctly by raising ValueError exception.",
        "type": "comment"
    },
    "162": {
        "file_id": 13,
        "content": "        RTX2(encoder_heads=-8)\n# Test case to check if RTX2 handles negative decoder heads correctly\ndef test_rtx2_negative_decoder_heads():\n    with pytest.raises(ValueError):\n        RTX2(decoder_heads=-8)",
        "type": "code",
        "location": "/tests.py:471-477"
    },
    "163": {
        "file_id": 13,
        "content": "The code tests if the RTX2 function handles negative decoder heads correctly by raising a ValueError when given a negative value for decoder_heads. It does so using pytest's raises context manager to ensure that the expected exception is raised.",
        "type": "comment"
    },
    "164": {
        "file_id": 14,
        "content": "/tests/test_data_utils.py",
        "type": "filepath"
    },
    "165": {
        "file_id": 14,
        "content": "The code includes tests for validating data processing operations and image preprocessing in the \"RT-X\" codebase, verifying correct handling of various inputs using test functions.",
        "type": "summary"
    },
    "166": {
        "file_id": 14,
        "content": "import io\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom rtx.data_util import describe, format_imgs, preprocess\ndef test_describe():\n    dic = {\n        \"key1\": \"value1\",\n        \"key2\": [1, 2, 3],\n        \"key3\": {\"nested_key\": \"nested_value\"},\n    }\n    describe(dic)\ndef test_describe_empty():\n    dic = {}\n    describe(dic)\ndef test_describe_non_dict():\n    non_dict = \"not a dict\"\n    describe(non_dict)\ndef test_preprocess():\n    dic = {\n        \"key1\": \"value1\",\n        \"key2\": [1, 2, 3],\n        \"key3\": {\"nested_key\": \"nested_value\"},\n    }\n    result = preprocess(dic)\n    assert result == dic\ndef test_preprocess_empty():\n    dic = {}\n    result = preprocess(dic)\n    assert result == dic\ndef test_preprocess_non_dict():\n    non_dict = \"not a dict\"\n    result = preprocess(non_dict)\n    assert result == non_dict\ndef test_preprocess_none_value():\n    dic = {\"key1\": None}\n    result = preprocess(dic)\n    assert result == {}\ndef test_preprocess_image():\n    img = Image.new(\"RGB\", (60, 30), color=\"red\")\n    img_byte_arr = io.BytesIO()",
        "type": "code",
        "location": "/tests/test_data_utils.py:1-59"
    },
    "167": {
        "file_id": 14,
        "content": "The code defines several test functions for validating data processing operations in the \"RT-X\" codebase. These tests involve checking if the provided dictionaries remain unchanged after preprocessing, ensuring correct handling of empty dictionaries, and verifying non-dictionary inputs return the same value. Additionally, an image object is created, indicating potential image preprocessing functionality within the codebase.",
        "type": "comment"
    },
    "168": {
        "file_id": 14,
        "content": "    img.save(img_byte_arr, format=\"PNG\")\n    img_byte_arr = img_byte_arr.getvalue()\n    result = preprocess(img_byte_arr)\n    assert isinstance(result, np.ndarray)\ndef test_format_imgs():\n    dic = {\n        \"key1\": \"value1\",\n        \"key2\": [1, 2, 3],\n        \"key3\": {\"nested_key\": \"nested_value\"},\n    }\n    result = format_imgs(dic, 224)\n    assert result == dic\ndef test_format_imgs_empty():\n    dic = {}\n    result = format_imgs(dic, 224)\n    assert result == dic\ndef test_format_imgs_non_dict():\n    non_dict = \"not a dict\"\n    result = format_imgs(non_dict, 224)\n    assert result == non_dict\ndef test_format_imgs_image():\n    img = Image.new(\"RGB\", (60, 30), color=\"red\")\n    img_byte_arr = io.BytesIO()\n    img.save(img_byte_arr, format=\"PNG\")\n    img_byte_arr = img_byte_arr.getvalue()\n    result = format_imgs(img_byte_arr, 224)\n    assert isinstance(result, np.ndarray)\ndef test_format_imgs_tensor():\n    tensor = torch.tensor([1, 2, 3])\n    result = format_imgs(tensor, 224)\n    assert isinstance(result, torch.Tensor)",
        "type": "code",
        "location": "/tests/test_data_utils.py:60-100"
    },
    "169": {
        "file_id": 14,
        "content": "The code contains multiple test functions for the `format_imgs` function. The tests include checking if the result is equal to the input dictionary (`test_format_imgs`), an empty dictionary (`test_format_imgs_empty`), non-dictionary inputs (`test_format_imgs_non_dict`), image inputs (`test_format_imgs_image`), and tensor inputs (`test_format_imgs_tensor`). It also checks if the result from an image input is a NumPy array (`test_format_imgs_image`).",
        "type": "comment"
    },
    "170": {
        "file_id": 14,
        "content": "def test_format_imgs_list():\n    list_val = [1, 2, 3]\n    result = format_imgs(list_val, 224)\n    assert result == list_val\ndef test_format_imgs_nested_dict():\n    dic = {\"key1\": {\"nested_key\": \"nested_value\"}}\n    result = format_imgs(dic, 224)\n    assert result == dic\ndef test_format_imgs_nested_list():\n    dic = {\"key1\": [1, 2, 3]}\n    result = format_imgs(dic, 224)\n    assert result == dic\ndef test_format_imgs_nested_image():\n    img = Image.new(\"RGB\", (60, 30), color=\"red\")\n    img_byte_arr = io.BytesIO()\n    img.save(img_byte_arr, format=\"PNG\")\n    img_byte_arr = img_byte_arr.getvalue()\n    dic = {\"key1\": img_byte_arr}\n    result = format_imgs(dic, 224)\n    assert isinstance(result[\"key1\"], np.ndarray)\ndef test_format_imgs_nested_tensor():\n    tensor = torch.tensor([1, 2, 3])\n    dic = {\"key1\": tensor}\n    result = format_imgs(dic, 224)\n    assert isinstance(result[\"key1\"], torch.Tensor)\ndef test_format_imgs_nested_list():\n    list_val = [1, 2, 3]\n    dic = {\"key1\": list_val}\n    result = format_imgs(dic, 224)",
        "type": "code",
        "location": "/tests/test_data_utils.py:103-141"
    },
    "171": {
        "file_id": 14,
        "content": "This code includes several test functions that demonstrate the functionality of a function called \"format_imgs\". The tests cover various scenarios such as handling lists, nested dictionaries, image bytes, and tensors. The purpose of these tests is to ensure the \"format_imgs\" function works correctly in different situations.",
        "type": "comment"
    },
    "172": {
        "file_id": 14,
        "content": "    assert result == dic",
        "type": "code",
        "location": "/tests/test_data_utils.py:142-142"
    },
    "173": {
        "file_id": 14,
        "content": "The code is checking if the result of an operation equals a dictionary (dic) and asserting it as a condition. This could be part of a testing process where the expected output is compared with the actual result to ensure correct functionality.",
        "type": "comment"
    },
    "174": {
        "file_id": 15,
        "content": "/tests/test_rtx1.py",
        "type": "filepath"
    },
    "175": {
        "file_id": 15,
        "content": "The code tests RTX1 model layers, evaluates logits with a condition scale of 3, and asserts shape equality. It is instantiated with video and instructions and wrapped in a conditional statement.",
        "type": "summary"
    },
    "176": {
        "file_id": 15,
        "content": "import unittest\nimport torch\nfrom rtx.rtx1 import RTX1, FilmViTConfig, RT1Config\nclass RTX1Test(unittest.TestCase):\n    def setUp(self):\n        self.batch_size = 2\n        self.num_frames = 6\n        self.num_actions = 11\n        self.num_action_bins = 256\n        self.video = torch.randn(\n            self.batch_size, 3, self.num_frames, 224, 224\n        )\n        self.instructions = [\n            \"bring me that apple sitting on the table\",\n            \"please pass the butter\",\n        ]\n        rt1_config = RT1Config(\n            num_actions=self.num_actions,\n            action_bins=self.num_action_bins,\n        )\n        self.rtx1 = RTX1(rt1_config)\n        self.rtx1_pretrained = RTX1(\n            rt1_config, FilmViTConfig(pretrained=True)\n        )\n        self.expected_logits_shape = torch.Size(\n            [\n                self.batch_size,\n                self.num_frames,\n                self.num_actions,\n                self.num_action_bins,\n            ]\n        )\n    def test_default_pretrained_has_same_shape(self):",
        "type": "code",
        "location": "/tests/test_rtx1.py:1-38"
    },
    "177": {
        "file_id": 15,
        "content": "The code imports necessary libraries and defines a test class for testing the RTX1 model. The setUp method initializes variables, creates instances of RTX1 and pretrained RTX1 models, and defines expected logits shape. It then sets up a test to check if default pretrained RTX1 has the same shape as the non-pretrained one.",
        "type": "comment"
    },
    "178": {
        "file_id": 15,
        "content": "        # Tests the general shape as the pretrained version from pytorch has\n        # different layernorm and conv2dnorm implementations.\n        assert len(self.rtx1.vit.layers) == len(\n            self.rtx1_pretrained.vit.layers\n        )\n    def test_default_train_eval(self):\n        train_logits = self.rtx1.train(self.video, self.instructions)\n        assert train_logits.shape == self.expected_logits_shape\n        self.rtx1.model.eval()\n        # compute the eval logits with a conditional scale of 3\n        eval_logits = self.rtx1.run(\n            self.video, self.instructions, cond_scale=3.0\n        )\n        assert eval_logits.shape == self.expected_logits_shape\n    def test_pretrained_train_eval(self):\n        train_logits = self.rtx1_pretrained.train(\n            self.video, self.instructions\n        )\n        assert train_logits.shape == self.expected_logits_shape\n        self.rtx1.model.eval()\n        # compute the eval logits with a conditional scale of 3\n        eval_logits = self.rtx1_pretrained.run(",
        "type": "code",
        "location": "/tests/test_rtx1.py:39-67"
    },
    "179": {
        "file_id": 15,
        "content": "This code tests the equality of layers in both rtx1 and pretrained rtx1 models, ensures that train and eval logits shapes match expected values, and checks if the eval logits are computed correctly with a conditional scale of 3.",
        "type": "comment"
    },
    "180": {
        "file_id": 15,
        "content": "            self.video, self.instructions, cond_scale=3.0\n        )\n        assert eval_logits.shape == self.expected_logits_shape\nif __name__ == \"__main__\":\n    unittest.main()",
        "type": "code",
        "location": "/tests/test_rtx1.py:68-74"
    },
    "181": {
        "file_id": 15,
        "content": "This code is instantiating an object with video and instructions, setting condition scale to 3.0, and asserting the shape of eval_logits matches the expected logits shape. The code block is wrapped inside a conditional statement ensuring it only runs if the file is run directly as a script, and includes a unit test main function call.",
        "type": "comment"
    }
}